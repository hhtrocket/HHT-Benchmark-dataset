{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 8. Evaluator Agent Benchmark (Comp-Analysis)\n",
    "**Category:** AI Agent Core Capabilities\n",
    "\n",
    "**Source:** [e0397123 / comp-analysis](https://github.com/e0397123/comp-analysis)\n",
    "\n",
    "**Description:** Used to train Critic Agents specifically for evaluating the\n",
    "dialogue quality generated by other agents.\n",
    "\n",
    "**Data Content:** Multi-dimensional dialogue evaluation data, including comparisons\n",
    "between various LLM scores for dialogue quality and human ratings.\n",
    "\n",
    "**Paper:** [Large Language Models Are Not Yet Human-Level Evaluators for Abstractive Summarization (AAAI 2024)](https://arxiv.org/abs/2305.13091)\n",
    "\n",
    "---\n",
    "\n",
    "**This notebook covers:**\n",
    "1. Data loading: dialog-level GPT-4 annotations, FED human annotations, turn-level ratings\n",
    "2. GPT-4 score distributions & inter-rater agreement\n",
    "3. Human annotation analysis (FED dataset)\n",
    "4. GPT-4 vs Human alignment comparison\n",
    "5. Dialog model ranking & turn-level cross-dataset comparison\n",
    "6. Turn-level dimension correlation heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"axes.titlesize\"] = 13\n",
    "plt.rcParams[\"axes.labelsize\"] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview\n",
    "\n",
    "This benchmark evaluates how well LLMs (GPT-4) can serve as automatic dialogue\n",
    "evaluators compared to human judges. Data is organized at two levels:\n",
    "\n",
    "**Dialog-level evaluation** (whole conversation quality):\n",
    "- 5 evaluation dimensions: Coherence, Diversity, Engagement, Informativeness, Overall\n",
    "- 5 independent GPT-4 raters per dimension\n",
    "- Human annotations with multiple annotators\n",
    "- Source datasets: FED, HEVAL, IEVAL, ConTurE, Reliable, PersonaSee\n",
    "\n",
    "**Turn-level evaluation** (single response quality):\n",
    "- 5 evaluation dimensions: Interesting, Relevance, Specificity, Understandability, Overall\n",
    "- Source datasets: FED-turn, ConTurE-turn, PersonaUSR, PersonaZhao, DailyDialog, TopicalUSR\n",
    "\n",
    "**Robustness tests:** Perturbed versions to test evaluator consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "### 3.1 Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (skip if already cloned)\n",
    "REPO_DIR = Path(\"comp-analysis\")\n",
    "if not REPO_DIR.exists():\n",
    "    os.system(\"git clone https://github.com/e0397123/comp-analysis.git\")\n",
    "    print(\"Repository cloned.\")\n",
    "else:\n",
    "    print(f\"Repository already exists at {REPO_DIR}\")\n",
    "\n",
    "DIALOG_DIR = REPO_DIR / \"dialog_level_texts\"\n",
    "TURN_DIR = REPO_DIR / \"turn_level_texts\"\n",
    "ROBUST_DIR = REPO_DIR / \"robustness_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 3.2 Load Dialog-Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dialog-level GPT-4 annotations\n",
    "gpt4_files = sorted(DIALOG_DIR.glob(\"*_gpt4_annotations.json\"))\n",
    "\n",
    "gpt4_dialog = {}\n",
    "for f in gpt4_files:\n",
    "    name = f.stem.replace(\"_gpt4_annotations\", \"\")\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "        gpt4_dialog[name] = json.load(fh)\n",
    "    n_dialogues = len(list(gpt4_dialog[name].values())[0])\n",
    "    n_cols = len(gpt4_dialog[name])\n",
    "    print(f\"  {name}: {n_dialogues} dialogues, {n_cols} columns\")\n",
    "\n",
    "print(f\"\\nTotal dialog-level datasets: {len(gpt4_dialog)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dialog-level human annotations (FED dataset)\n",
    "human_file = DIALOG_DIR / \"fed_human_annotations.json\"\n",
    "with open(human_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    fed_human = json.load(f)\n",
    "\n",
    "print(f\"FED human annotations: {len(fed_human)} dialogues\")\n",
    "print(f\"Fields per dialogue: {list(fed_human[0].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### 3.3 Load Turn-Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load turn-level ratings\n",
    "turn_files = sorted(TURN_DIR.glob(\"turn_*_ratings.json\"))\n",
    "\n",
    "turn_ratings = {}\n",
    "for f in turn_files:\n",
    "    name = f.stem.replace(\"_ratings\", \"\")\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "        turn_ratings[name] = json.load(fh)\n",
    "    datasets_in = list(turn_ratings[name].keys())\n",
    "    n_items = len(list(turn_ratings[name].values())[0])\n",
    "    print(f\"  {name}: {len(datasets_in)} source datasets, {n_items} items each\")\n",
    "\n",
    "print(f\"\\nTotal turn-level rating files: {len(turn_ratings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 4. Data Schema & Samples\n",
    "\n",
    "### 4.1 Dialog-Level GPT-4 Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_gpt4 = gpt4_dialog[\"fed\"]\n",
    "\n",
    "print(\"=== Dialog-Level GPT-4 Annotations (FED) ===\")\n",
    "print(f\"Columns: {list(fed_gpt4.keys())}\")\n",
    "print(f\"Number of dialogues: {len(list(fed_gpt4.values())[0])}\")\n",
    "print(f\"\\nDimension naming: <dim>_<rater_id>\")\n",
    "print(\"  coh = Coherence, div = Diversity, eng = Engagement,\")\n",
    "print(\"  inf = Informativeness, ovr = Overall\")\n",
    "print(f\"\\nSample scores (first 5 dialogues, rater 1):\")\n",
    "for dim in [\"coh_1\", \"div_1\", \"eng_1\", \"inf_1\", \"ovr_1\"]:\n",
    "    print(f\"  {dim}: {fed_gpt4[dim][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### 4.2 Dialog-Level Human Annotations (FED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = fed_human[0]\n",
    "print(\"=== Dialog-Level Human Annotations (FED) ===\")\n",
    "print(f\"Keys: {list(sample.keys())}\")\n",
    "print(f\"Model: {sample['model']}\")\n",
    "print(f\"Dialogue ID: {sample['dialogue_id']}\")\n",
    "print(f\"Dialog turns: {len(sample['dialog'])}\")\n",
    "print(f\"Annotation dimensions: {list(sample['annotations'].keys())}\")\n",
    "\n",
    "print(f\"\\nSample dialog:\")\n",
    "for turn in sample[\"dialog\"][:4]:\n",
    "    print(f\"  [{turn['speaker']}]: {turn['text'][:80]}...\")\n",
    "\n",
    "print(f\"\\nSample annotations:\")\n",
    "for dim, scores in list(sample[\"annotations\"].items())[:5]:\n",
    "    print(f\"  {dim}: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### 4.3 Turn-Level Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Turn-Level Ratings ===\")\n",
    "print(f\"Rating files: {list(turn_ratings.keys())}\")\n",
    "for name, data in turn_ratings.items():\n",
    "    first_ds = list(data.keys())[0]\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Source datasets: {list(data.keys())}\")\n",
    "    print(f\"    Sample ({first_ds}, first 5): {data[first_ds][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "### 5.1 GPT-4 Score Distribution by Dimension (Dialog-Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = {\n",
    "    \"coh\": \"Coherence\", \"div\": \"Diversity\", \"eng\": \"Engagement\",\n",
    "    \"inf\": \"Informativeness\", \"ovr\": \"Overall\",\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for dim_key, dim_name in dimensions.items():\n",
    "    for rater in range(1, 6):\n",
    "        col = f\"{dim_key}_{rater}\"\n",
    "        if col in fed_gpt4:\n",
    "            for score in fed_gpt4[col]:\n",
    "                rows.append({\"dimension\": dim_name, \"rater\": rater, \"score\": score})\n",
    "\n",
    "df_gpt4 = pd.DataFrame(rows)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_gpt4, x=\"dimension\", y=\"score\",\n",
    "            hue=\"dimension\", palette=\"Set2\", legend=False)\n",
    "plt.title(\"GPT-4 Score Distribution by Evaluation Dimension (FED Dialog-Level)\")\n",
    "plt.xlabel(\"Dimension\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"GPT-4 score statistics per dimension:\")\n",
    "print(df_gpt4.groupby(\"dimension\")[\"score\"].describe().round(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### 5.2 Inter-Rater Agreement (GPT-4 Raters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and std across 5 GPT-4 raters for each dialogue\n",
    "rater_agreement = {}\n",
    "for dim_key, dim_name in dimensions.items():\n",
    "    rater_cols = [f\"{dim_key}_{r}\" for r in range(1, 6)\n",
    "                  if f\"{dim_key}_{r}\" in fed_gpt4]\n",
    "    if rater_cols:\n",
    "        rater_matrix = np.array([fed_gpt4[c] for c in rater_cols])  # (5, N)\n",
    "        rater_agreement[dim_name] = {\n",
    "            \"mean_std\": np.mean(np.std(rater_matrix, axis=0)),\n",
    "            \"mean_range\": np.mean(np.max(rater_matrix, axis=0)\n",
    "                                  - np.min(rater_matrix, axis=0)),\n",
    "        }\n",
    "\n",
    "agree_df = pd.DataFrame(rater_agreement).T\n",
    "agree_df.columns = [\"Mean Std (across raters)\", \"Mean Range (across raters)\"]\n",
    "\n",
    "agree_df.plot(kind=\"bar\", figsize=(10, 5), color=[\"steelblue\", \"coral\"])\n",
    "plt.title(\"GPT-4 Inter-Rater Disagreement by Dimension\")\n",
    "plt.ylabel(\"Score Variation\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(agree_df.round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### 5.3 Human Annotation Distribution (FED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract human ratings per dimension\n",
    "human_dims = {}\n",
    "for item in fed_human:\n",
    "    for dim, scores in item[\"annotations\"].items():\n",
    "        if dim not in human_dims:\n",
    "            human_dims[dim] = []\n",
    "        human_dims[dim].extend(scores)\n",
    "\n",
    "dim_counts = {k: len(v) for k, v in human_dims.items()}\n",
    "dim_df = pd.Series(dim_counts).sort_values(ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "dim_df.head(11).plot(kind=\"barh\", ax=axes[0], color=\"steelblue\")\n",
    "axes[0].set_title(\"Number of Human Ratings per Dimension\")\n",
    "axes[0].set_xlabel(\"Total Ratings\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "for dim in [\"Coherent\", \"Overall\", \"Informative\", \"Likeable\"]:\n",
    "    if dim in human_dims:\n",
    "        axes[1].hist(human_dims[dim], bins=range(0, 7), alpha=0.5,\n",
    "                     label=dim, edgecolor=\"white\")\n",
    "axes[1].set_title(\"Human Rating Distribution (Selected Dimensions)\")\n",
    "axes[1].set_xlabel(\"Rating\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### 5.4 GPT-4 vs Human: FED Dataset Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-dialogue mean GPT-4 scores for 'Overall'\n",
    "gpt4_overall = []\n",
    "for rater in range(1, 6):\n",
    "    col = f\"ovr_{rater}\"\n",
    "    if col in fed_gpt4:\n",
    "        gpt4_overall.append(fed_gpt4[col])\n",
    "gpt4_overall_mean = np.mean(gpt4_overall, axis=0)\n",
    "\n",
    "# Compute per-dialogue mean human 'Overall' scores\n",
    "human_overall = []\n",
    "for item in fed_human:\n",
    "    if \"Overall\" in item[\"annotations\"]:\n",
    "        human_overall.append(np.mean(item[\"annotations\"][\"Overall\"]))\n",
    "    else:\n",
    "        human_overall.append(np.nan)\n",
    "\n",
    "# Align lengths and remove NaN\n",
    "min_len = min(len(gpt4_overall_mean), len(human_overall))\n",
    "gpt4_aligned = gpt4_overall_mean[:min_len]\n",
    "human_aligned = np.array(human_overall[:min_len])\n",
    "mask = ~np.isnan(human_aligned)\n",
    "gpt4_clean = gpt4_aligned[mask]\n",
    "human_clean = human_aligned[mask]\n",
    "\n",
    "if len(gpt4_clean) > 0:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(human_clean, gpt4_clean, alpha=0.5, s=40,\n",
    "                color=\"steelblue\", edgecolors=\"black\")\n",
    "    plt.xlabel(\"Human Mean Overall Rating\")\n",
    "    plt.ylabel(\"GPT-4 Mean Overall Rating\")\n",
    "    plt.title(\"GPT-4 vs Human: Per-Dialogue Overall Rating\")\n",
    "\n",
    "    lims = [min(plt.xlim()[0], plt.ylim()[0]),\n",
    "            max(plt.xlim()[1], plt.ylim()[1])]\n",
    "    plt.plot(lims, lims, \"--\", color=\"gray\", alpha=0.5,\n",
    "             label=\"Perfect agreement\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    correlation = np.corrcoef(human_clean, gpt4_clean)[0, 1]\n",
    "    print(f\"Pearson correlation (Human vs GPT-4 Overall): {correlation:.3f}\")\n",
    "    print(f\"Number of dialogues compared: {len(gpt4_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### 5.5 Dialog Models Compared (Human Annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "for item in fed_human:\n",
    "    model = item.get(\"model\", \"unknown\")\n",
    "    if \"Overall\" in item[\"annotations\"]:\n",
    "        if model not in model_scores:\n",
    "            model_scores[model] = []\n",
    "        model_scores[model].append(np.mean(item[\"annotations\"][\"Overall\"]))\n",
    "\n",
    "model_df = pd.DataFrame([\n",
    "    {\"Model\": m, \"Mean Overall\": np.mean(s), \"Std\": np.std(s), \"Count\": len(s)}\n",
    "    for m, s in model_scores.items()\n",
    "]).sort_values(\"Mean Overall\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.barh(model_df[\"Model\"], model_df[\"Mean Overall\"],\n",
    "                color=\"mediumseagreen\", edgecolor=\"white\",\n",
    "                xerr=model_df[\"Std\"], capsize=3)\n",
    "plt.title(\"Human Overall Rating by Dialog Model (FED)\")\n",
    "plt.xlabel(\"Mean Overall Rating\")\n",
    "plt.gca().invert_yaxis()\n",
    "for bar, count in zip(bars, model_df[\"Count\"]):\n",
    "    plt.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height() / 2,\n",
    "             f\"n={count}\", va=\"center\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### 5.6 Turn-Level: Cross-Dataset Rating Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"turn_overall\" in turn_ratings:\n",
    "    overall = turn_ratings[\"turn_overall\"]\n",
    "    turn_stats = {}\n",
    "    for ds_name, ratings in overall.items():\n",
    "        ratings_clean = [r for r in ratings\n",
    "                         if r is not None\n",
    "                         and not (isinstance(r, float) and np.isnan(r))]\n",
    "        if ratings_clean:\n",
    "            turn_stats[ds_name] = {\n",
    "                \"mean\": np.mean(ratings_clean),\n",
    "                \"std\": np.std(ratings_clean),\n",
    "                \"count\": len(ratings_clean),\n",
    "            }\n",
    "\n",
    "    turn_df = pd.DataFrame(turn_stats).T.sort_values(\"mean\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(turn_df.index, turn_df[\"mean\"], color=\"orchid\",\n",
    "             edgecolor=\"white\", xerr=turn_df[\"std\"], capsize=3)\n",
    "    plt.title(\"Turn-Level GPT-4 Overall Rating by Source Dataset\")\n",
    "    plt.xlabel(\"Mean Overall Rating\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Turn-level overall statistics:\")\n",
    "    print(turn_df.round(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### 5.7 Turn-Level: Dimension Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a per-item DataFrame with all turn-level dimensions\n",
    "# Use a persona dataset as reference (exists in most rating files)\n",
    "turn_dim_data = {}\n",
    "for name, data in turn_ratings.items():\n",
    "    dim_name = name.replace(\"turn_\", \"\").capitalize()\n",
    "    for ds_key in data.keys():\n",
    "        if \"persona\" in ds_key.lower():\n",
    "            turn_dim_data[dim_name] = data[ds_key]\n",
    "            break\n",
    "\n",
    "if len(turn_dim_data) >= 3:\n",
    "    min_len = min(len(v) for v in turn_dim_data.values())\n",
    "    turn_dim_df = pd.DataFrame({k: v[:min_len] for k, v in turn_dim_data.items()})\n",
    "    turn_dim_df = turn_dim_df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "\n",
    "    if len(turn_dim_df) > 10:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        corr = turn_dim_df.corr()\n",
    "        sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\",\n",
    "                    vmin=-1, vmax=1, square=True, linewidths=0.5)\n",
    "        plt.title(\"Turn-Level Dimension Correlation (Persona dataset)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### 5.8 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Dataset Summary ===\\n\")\n",
    "\n",
    "print(f\"Dialog-level GPT-4 annotation sets: {len(gpt4_dialog)}\")\n",
    "for name, data in gpt4_dialog.items():\n",
    "    n = len(list(data.values())[0])\n",
    "    print(f\"  {name}: {n} dialogues, {len(data)} columns (5 dims x 5 raters)\")\n",
    "\n",
    "print(f\"\\nDialog-level human annotations (FED): {len(fed_human)} dialogues\")\n",
    "print(f\"  Models: {sorted(set(d.get('model', '?') for d in fed_human))}\")\n",
    "\n",
    "print(f\"\\nTurn-level rating files: {len(turn_ratings)}\")\n",
    "for name, data in turn_ratings.items():\n",
    "    print(f\"  {name}: {list(data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## 6. Key Observations\n",
    "\n",
    "1. **Multi-dimensional evaluation:** The benchmark covers 5+ quality dimensions\n",
    "   (coherence, diversity, engagement, informativeness, overall) at both dialog\n",
    "   and turn levels, enabling fine-grained evaluator assessment.\n",
    "\n",
    "2. **GPT-4 inter-rater variability:** Even with the same model, 5 independent\n",
    "   GPT-4 runs produce different scores, highlighting the stochastic nature\n",
    "   of LLM-based evaluation.\n",
    "\n",
    "3. **Human-LLM alignment gap:** The correlation between GPT-4 and human\n",
    "   ratings varies by dimension, suggesting LLMs are better evaluators on some\n",
    "   quality aspects than others.\n",
    "\n",
    "4. **Cross-dataset generalization:** Turn-level ratings vary significantly\n",
    "   across source datasets, indicating that evaluator quality is task-dependent.\n",
    "\n",
    "5. **Research relevance (IS/AI):**\n",
    "   - **Critic agents:** Train LLMs to evaluate other agents' outputs reliably\n",
    "   - **Evaluation automation:** Replace expensive human evaluation with calibrated LLM judges\n",
    "   - **Bias detection:** Identify systematic differences between human and LLM ratings\n",
    "   - **Multi-dimensional quality:** Move beyond single-score evaluation to nuanced assessment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}