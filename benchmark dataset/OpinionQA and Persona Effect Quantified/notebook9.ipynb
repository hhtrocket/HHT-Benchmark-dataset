{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Evaluator Agent Benchmark — Deep Analysis (Comp-Analysis)\n",
    "**Category:** AI Agent Core Capabilities\n",
    "\n",
    "**Source:** [e0397123 / comp-analysis](https://github.com/e0397123/comp-analysis)\n",
    "\n",
    "**Description:** Used to train Critic Agents specifically for evaluating the\n",
    "dialogue quality generated by other agents.\n",
    "\n",
    "**Data Content:** Multi-dimensional dialogue evaluation data, comparing LLM scores\n",
    "with human ratings across dialog-level and turn-level quality dimensions.\n",
    "\n",
    "**Paper:** [A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators (AAAI 2024)](https://ojs.aaai.org/index.php/AAAI/article/view/29918)\n",
    "\n",
    "---\n",
    "\n",
    "**This notebook extends notebook 8** by performing:\n",
    "1. Spearman & Pearson correlation analysis across all datasets and dimensions\n",
    "2. GPT-4 evaluator calibration and bias analysis\n",
    "3. Robustness testing (original vs perturbed dialogues)\n",
    "4. Cross-dataset evaluator generalization\n",
    "5. A critic agent evaluation scoring framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install pandas matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom scipy import stats\nfrom itertools import combinations\n\nsns.set_theme(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (12, 6)\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"axes.titlesize\"] = 13\nplt.rcParams[\"axes.labelsize\"] = 11\n\n# Helper: some GPT-4 JSON files contain an extra \"indices\" column that is NOT\n# a score column.  This function returns only the actual score column names\n# (pattern: <dim>_<rater>, e.g. \"coh_1\", \"ovr_5\").\nimport re as _re\n_SCORE_COL_RE = _re.compile(r\"^[a-z]+_\\d+$\")\n\ndef score_columns(data: dict) -> list:\n    \"\"\"Return only the score column names from a GPT-4 annotation dict.\"\"\"\n    return [c for c in data if _SCORE_COL_RE.match(c)]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview\n",
    "\n",
    "The **comp-analysis** benchmark evaluates LLMs as automatic dialogue evaluators.\n",
    "It provides three data categories:\n",
    "\n",
    "| Category | Level | Dimensions | Purpose |\n",
    "|----------|-------|------------|---------|\n",
    "| Dialog-level GPT-4 | Whole conversation | Coherence, Diversity, Engagement, Informativeness, Overall | Measure LLM evaluator scores |\n",
    "| Dialog-level Human | Whole conversation | 11 fine-grained dimensions (0–2 scale) | Ground truth for calibration |\n",
    "| Turn-level Ratings | Single response | Interesting, Relevance, Specificity, Understandability, Overall | Cross-dataset comparison |\n",
    "| Robustness Data | Both levels | Perturbation types: order shuffle, repetition, contradiction, boring | Test evaluator consistency |\n",
    "\n",
    "**Source datasets:** FED, HEVAL, IEVAL, ConTurE, Reliable, PersonaSee (dialog); FED-turn, ConTurE-turn, PersonaUSR, PersonaZhao, DailyDialog, TopicalUSR (turn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (skip if already cloned)\n",
    "REPO_DIR = Path(\"comp-analysis\")\n",
    "if not REPO_DIR.exists():\n",
    "    os.system(\"git clone https://github.com/e0397123/comp-analysis.git\")\n",
    "    print(\"Repository cloned.\")\n",
    "else:\n",
    "    print(f\"Repository already exists at {REPO_DIR}\")\n",
    "\n",
    "DIALOG_DIR = REPO_DIR / \"dialog_level_texts\"\n",
    "TURN_DIR = REPO_DIR / \"turn_level_texts\"\n",
    "ROBUST_DIR = REPO_DIR / \"robustness_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dialog-level GPT-4 annotations ---\n",
    "gpt4_files = sorted(DIALOG_DIR.glob(\"*_gpt4_annotations.json\"))\n",
    "\n",
    "gpt4_dialog = {}\n",
    "for f in gpt4_files:\n",
    "    name = f.stem.replace(\"_gpt4_annotations\", \"\")\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "        gpt4_dialog[name] = json.load(fh)\n",
    "    n_dialogues = len(list(gpt4_dialog[name].values())[0])\n",
    "    print(f\"  {name}: {n_dialogues} dialogues, {len(gpt4_dialog[name])} columns\")\n",
    "\n",
    "# --- Dialog-level human annotations (FED) ---\n",
    "human_file = DIALOG_DIR / \"fed_human_annotations.json\"\n",
    "with open(human_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    fed_human = json.load(f)\n",
    "print(f\"  FED human: {len(fed_human)} dialogues, \"\n",
    "      f\"dims = {list(fed_human[0]['annotations'].keys())}\")\n",
    "\n",
    "# --- Turn-level ratings ---\n",
    "turn_files = sorted(TURN_DIR.glob(\"turn_*_ratings.json\"))\n",
    "\n",
    "turn_ratings = {}\n",
    "for f in turn_files:\n",
    "    name = f.stem.replace(\"_ratings\", \"\")\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "        turn_ratings[name] = json.load(fh)\n",
    "    datasets_in = list(turn_ratings[name].keys())\n",
    "    print(f\"  {name}: {len(datasets_in)} datasets\")\n",
    "\n",
    "# --- Dialog-level text files (for dialogue content) ---\n",
    "dialog_texts = {}\n",
    "for f in sorted(DIALOG_DIR.glob(\"*_text.txt\")):\n",
    "    name = f.stem.replace(\"-dial_text\", \"\").replace(\"_text\", \"\")\n",
    "    with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "        lines = [l.strip() for l in fh if l.strip()]\n",
    "    dialog_texts[name] = lines\n",
    "    print(f\"  dialog text {name}: {len(lines)} dialogues\")\n",
    "\n",
    "print(f\"\\nLoaded {len(gpt4_dialog)} GPT-4 sets, {len(turn_ratings)} turn-level sets, \"\n",
    "      f\"{len(dialog_texts)} dialog text files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robustness data ---\n",
    "robust_dialog_dir = ROBUST_DIR / \"dialog_level_robust_data\"\n",
    "robust_turn_dir = ROBUST_DIR / \"turn_level_robust_data\"\n",
    "\n",
    "# Load dialog-level original coherent dialogues\n",
    "fed_originals = []\n",
    "orig_file = robust_dialog_dir / \"fed_coherent_original.json\"\n",
    "if orig_file.exists():\n",
    "    with open(orig_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        fed_originals = json.load(f)\n",
    "    print(f\"FED original dialogues: {len(fed_originals)}\")\n",
    "\n",
    "# Load all robustness perturbation text files\n",
    "robustness_data = {\"dialog\": {}, \"turn\": {}}\n",
    "\n",
    "if robust_dialog_dir.exists():\n",
    "    for dim_dir in sorted(robust_dialog_dir.iterdir()):\n",
    "        if dim_dir.is_dir():\n",
    "            dim_name = dim_dir.name\n",
    "            robustness_data[\"dialog\"][dim_name] = {}\n",
    "            for txt_file in sorted(dim_dir.glob(\"*.txt\")):\n",
    "                with open(txt_file, \"r\", encoding=\"utf-8\") as fh:\n",
    "                    lines = [l.strip() for l in fh if l.strip()]\n",
    "                robustness_data[\"dialog\"][dim_name][txt_file.stem] = lines\n",
    "\n",
    "if robust_turn_dir.exists():\n",
    "    for dim_dir in sorted(robust_turn_dir.iterdir()):\n",
    "        if dim_dir.is_dir():\n",
    "            dim_name = dim_dir.name\n",
    "            robustness_data[\"turn\"][dim_name] = {}\n",
    "            for txt_file in sorted(dim_dir.glob(\"*.txt\")):\n",
    "                with open(txt_file, \"r\", encoding=\"utf-8\") as fh:\n",
    "                    lines = [l.strip() for l in fh if l.strip()]\n",
    "                robustness_data[\"turn\"][dim_name][txt_file.stem] = lines\n",
    "\n",
    "print(f\"\\nRobustness (dialog-level) dimensions: {list(robustness_data['dialog'].keys())}\")\n",
    "for dim, files in robustness_data[\"dialog\"].items():\n",
    "    print(f\"  {dim}: {list(files.keys())} ({[len(v) for v in files.values()]} items)\")\n",
    "\n",
    "print(f\"\\nRobustness (turn-level) dimensions: {list(robustness_data['turn'].keys())}\")\n",
    "for dim, files in robustness_data[\"turn\"].items():\n",
    "    print(f\"  {dim}: {list(files.keys())} ({[len(v) for v in files.values()]} items)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Schema Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a summary of all GPT-4 annotation datasets\n",
    "dimensions = {\"coh\": \"Coherence\", \"div\": \"Diversity\", \"eng\": \"Engagement\",\n",
    "              \"inf\": \"Informativeness\", \"ovr\": \"Overall\"}\n",
    "\n",
    "print(\"=== GPT-4 Dialog-Level Annotation Schema ===\")\n",
    "print(f\"Dimensions: {list(dimensions.values())}\")\n",
    "print(f\"Raters per dimension: 5 (independent GPT-4 runs)\")\n",
    "print(f\"Score scale: 1-5 (float)\\n\")\n",
    "\n",
    "summary_rows = []\n",
    "for ds_name, data in gpt4_dialog.items():\n",
    "    n = len(list(data.values())[0])\n",
    "    dims_present = set()\n",
    "    for col in data.keys():\n",
    "        dim_key = col.rsplit(\"_\", 1)[0]\n",
    "        dims_present.add(dim_key)\n",
    "    summary_rows.append({\n",
    "        \"Dataset\": ds_name,\n",
    "        \"Dialogues\": n,\n",
    "        \"Columns\": len(data),\n",
    "        \"Dimensions\": \", \".join(sorted(dims_present)),\n",
    "    })\n",
    "\n",
    "pd.DataFrame(summary_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human annotation schema (11 fine-grained dimensions)\n",
    "print(\"=== Human Annotation Schema (FED) ===\")\n",
    "print(f\"Scale: 0-2 (integer, higher = better)\")\n",
    "print(f\"Annotators: multiple per dimension\\n\")\n",
    "\n",
    "human_dim_stats = {}\n",
    "for item in fed_human:\n",
    "    for dim, scores in item[\"annotations\"].items():\n",
    "        if dim not in human_dim_stats:\n",
    "            human_dim_stats[dim] = []\n",
    "        human_dim_stats[dim].extend(scores)\n",
    "\n",
    "human_summary = pd.DataFrame([\n",
    "    {\n",
    "        \"Dimension\": dim,\n",
    "        \"Total Ratings\": len(scores),\n",
    "        \"Mean\": np.mean(scores),\n",
    "        \"Std\": np.std(scores),\n",
    "        \"Unique Values\": sorted(set(scores)),\n",
    "    }\n",
    "    for dim, scores in human_dim_stats.items()\n",
    "]).sort_values(\"Total Ratings\", ascending=False)\n",
    "\n",
    "print(human_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialog text structure\n",
    "print(\"=== Dialog Text Format ===\")\n",
    "for ds_name, lines in list(dialog_texts.items())[:2]:\n",
    "    print(f\"\\n--- {ds_name} (first dialogue) ---\")\n",
    "    parts = lines[0].split(\"\\t\")\n",
    "    print(f\"  Dialog ID: {parts[0]}\")\n",
    "    print(f\"  Turns: {len(parts) - 1}\")\n",
    "    for turn in parts[1:4]:\n",
    "        print(f\"    {turn[:100]}\")\n",
    "    if len(parts) > 4:\n",
    "        print(f\"    ... ({len(parts) - 4} more turns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Dataset Correlation Analysis\n",
    "\n",
    "We compute **Spearman** (rank-based) and **Pearson** (linear) correlations between\n",
    "GPT-4 raters and between GPT-4 vs. human ratings. This is the core metric for\n",
    "evaluating LLMs as critic agents.\n",
    "\n",
    "### 5.1 GPT-4 Intra-Model Agreement (All Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise Spearman correlation between 5 GPT-4 raters for each dataset/dimension\n",
    "intra_results = []\n",
    "for ds_name, data in gpt4_dialog.items():\n",
    "    for dim_key, dim_label in dimensions.items():\n",
    "        rater_cols = [f\"{dim_key}_{r}\" for r in range(1, 6) if f\"{dim_key}_{r}\" in data]\n",
    "        if len(rater_cols) < 2:\n",
    "            continue\n",
    "        pair_corrs = []\n",
    "        for c1, c2 in combinations(rater_cols, 2):\n",
    "            rho, pval = stats.spearmanr(data[c1], data[c2])\n",
    "            pair_corrs.append(rho)\n",
    "        intra_results.append({\n",
    "            \"Dataset\": ds_name,\n",
    "            \"Dimension\": dim_label,\n",
    "            \"Mean Spearman\": np.mean(pair_corrs),\n",
    "            \"Std Spearman\": np.std(pair_corrs),\n",
    "            \"Min Spearman\": np.min(pair_corrs),\n",
    "            \"Max Spearman\": np.max(pair_corrs),\n",
    "            \"N Pairs\": len(pair_corrs),\n",
    "        })\n",
    "\n",
    "df_intra = pd.DataFrame(intra_results)\n",
    "print(\"=== GPT-4 Intra-Model Agreement (Pairwise Spearman) ===\")\n",
    "print(df_intra.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Mean Spearman correlation by Dataset x Dimension\n",
    "pivot_intra = df_intra.pivot(index=\"Dataset\", columns=\"Dimension\", values=\"Mean Spearman\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(pivot_intra, annot=True, fmt=\".3f\", cmap=\"YlOrRd\", vmin=0, vmax=1,\n",
    "            linewidths=0.5, square=True)\n",
    "plt.title(\"GPT-4 Intra-Rater Agreement (Mean Pairwise Spearman)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHighest agreement:\",\n",
    "      df_intra.loc[df_intra[\"Mean Spearman\"].idxmax()][\n",
    "          [\"Dataset\", \"Dimension\", \"Mean Spearman\"]].to_dict())\n",
    "print(\"Lowest agreement:\",\n",
    "      df_intra.loc[df_intra[\"Mean Spearman\"].idxmin()][\n",
    "          [\"Dataset\", \"Dimension\", \"Mean Spearman\"]].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 GPT-4 vs Human: Dimension-Level Correlation (FED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map human annotation dimensions to GPT-4 dimensions\n",
    "# Human: Coherent, Diverse, Likeable (~ Engagement), Informative, Overall\n",
    "# GPT-4: coh, div, eng, inf, ovr\n",
    "dim_mapping = {\n",
    "    \"coh\": \"Coherent\",\n",
    "    \"div\": \"Diverse\",\n",
    "    \"eng\": \"Likeable\",       # closest proxy for engagement\n",
    "    \"inf\": \"Informative\",\n",
    "    \"ovr\": \"Overall\",\n",
    "}\n",
    "\n",
    "fed_gpt4 = gpt4_dialog.get(\"fed\", {})\n",
    "\n",
    "# Compute per-dialogue mean for GPT-4 (mean of 5 raters) and human (mean of annotators)\n",
    "gpt4_vs_human = []\n",
    "for gpt4_dim, human_dim in dim_mapping.items():\n",
    "    # GPT-4 mean across 5 raters\n",
    "    rater_scores = []\n",
    "    for r in range(1, 6):\n",
    "        col = f\"{gpt4_dim}_{r}\"\n",
    "        if col in fed_gpt4:\n",
    "            rater_scores.append(fed_gpt4[col])\n",
    "    if not rater_scores:\n",
    "        continue\n",
    "    gpt4_mean = np.mean(rater_scores, axis=0)\n",
    "\n",
    "    # Human mean per dialogue\n",
    "    human_mean = []\n",
    "    for item in fed_human:\n",
    "        if human_dim in item[\"annotations\"]:\n",
    "            human_mean.append(np.mean(item[\"annotations\"][human_dim]))\n",
    "        else:\n",
    "            human_mean.append(np.nan)\n",
    "\n",
    "    # Align and clean\n",
    "    min_len = min(len(gpt4_mean), len(human_mean))\n",
    "    g = gpt4_mean[:min_len]\n",
    "    h = np.array(human_mean[:min_len])\n",
    "    mask = ~np.isnan(h)\n",
    "    g_clean, h_clean = g[mask], h[mask]\n",
    "\n",
    "    if len(g_clean) >= 5:\n",
    "        spearman_r, spearman_p = stats.spearmanr(h_clean, g_clean)\n",
    "        pearson_r, pearson_p = stats.pearsonr(h_clean, g_clean)\n",
    "        gpt4_vs_human.append({\n",
    "            \"GPT-4 Dim\": dimensions[gpt4_dim],\n",
    "            \"Human Dim\": human_dim,\n",
    "            \"Spearman r\": spearman_r,\n",
    "            \"Spearman p\": spearman_p,\n",
    "            \"Pearson r\": pearson_r,\n",
    "            \"Pearson p\": pearson_p,\n",
    "            \"N\": len(g_clean),\n",
    "        })\n",
    "\n",
    "df_corr = pd.DataFrame(gpt4_vs_human)\n",
    "print(\"=== GPT-4 vs Human Correlation (FED Dataset) ===\")\n",
    "print(df_corr.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GPT-4 vs Human correlations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x = range(len(df_corr))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar([i - width/2 for i in x], df_corr[\"Spearman r\"], width,\n",
    "            label=\"Spearman\", color=\"steelblue\")\n",
    "axes[0].bar([i + width/2 for i in x], df_corr[\"Pearson r\"], width,\n",
    "            label=\"Pearson\", color=\"coral\")\n",
    "axes[0].set_xticks(list(x))\n",
    "axes[0].set_xticklabels(df_corr[\"GPT-4 Dim\"], rotation=15)\n",
    "axes[0].set_ylabel(\"Correlation\")\n",
    "axes[0].set_title(\"GPT-4 vs Human: Correlation by Dimension\")\n",
    "axes[0].legend()\n",
    "axes[0].axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Significance markers\n",
    "for i, (_, row) in enumerate(df_corr.iterrows()):\n",
    "    sig = (\"***\" if row[\"Spearman p\"] < 0.001 else\n",
    "           \"**\" if row[\"Spearman p\"] < 0.01 else\n",
    "           \"*\" if row[\"Spearman p\"] < 0.05 else \"ns\")\n",
    "    axes[0].text(i - width/2, row[\"Spearman r\"] + 0.02, sig,\n",
    "                 ha=\"center\", fontsize=8)\n",
    "\n",
    "# Scatter plots for each dimension\n",
    "colors = [\"steelblue\", \"coral\", \"mediumseagreen\", \"orchid\", \"goldenrod\"]\n",
    "for idx, (gpt4_dim, human_dim) in enumerate(dim_mapping.items()):\n",
    "    rater_scores = [fed_gpt4[f\"{gpt4_dim}_{r}\"] for r in range(1, 6)\n",
    "                    if f\"{gpt4_dim}_{r}\" in fed_gpt4]\n",
    "    if not rater_scores:\n",
    "        continue\n",
    "    gpt4_mean = np.mean(rater_scores, axis=0)\n",
    "    human_mean = [\n",
    "        np.mean(item[\"annotations\"][human_dim])\n",
    "        if human_dim in item[\"annotations\"] else np.nan\n",
    "        for item in fed_human\n",
    "    ]\n",
    "    min_len = min(len(gpt4_mean), len(human_mean))\n",
    "    g, h = gpt4_mean[:min_len], np.array(human_mean[:min_len])\n",
    "    mask = ~np.isnan(h)\n",
    "    axes[1].scatter(h[mask], g[mask], alpha=0.5, s=30,\n",
    "                    label=dimensions[gpt4_dim], color=colors[idx])\n",
    "\n",
    "axes[1].set_xlabel(\"Human Mean Rating\")\n",
    "axes[1].set_ylabel(\"GPT-4 Mean Rating\")\n",
    "axes[1].set_title(\"GPT-4 vs Human: All Dimensions\")\n",
    "axes[1].legend(fontsize=8)\n",
    "lims = [min(axes[1].get_xlim()[0], axes[1].get_ylim()[0]),\n",
    "        max(axes[1].get_xlim()[1], axes[1].get_ylim()[1])]\n",
    "axes[1].plot(lims, lims, \"--\", color=\"gray\", alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Cross-Dataset GPT-4 Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GPT-4 score distributions across all datasets for each dimension\n",
    "cross_rows = []\n",
    "for ds_name, data in gpt4_dialog.items():\n",
    "    for dim_key, dim_label in dimensions.items():\n",
    "        all_scores = []\n",
    "        for r in range(1, 6):\n",
    "            col = f\"{dim_key}_{r}\"\n",
    "            if col in data:\n",
    "                all_scores.extend(data[col])\n",
    "        if all_scores:\n",
    "            cross_rows.append({\n",
    "                \"Dataset\": ds_name,\n",
    "                \"Dimension\": dim_label,\n",
    "                \"Mean\": np.mean(all_scores),\n",
    "                \"Std\": np.std(all_scores),\n",
    "                \"Median\": np.median(all_scores),\n",
    "                \"Skewness\": stats.skew(all_scores),\n",
    "                \"N\": len(all_scores),\n",
    "            })\n",
    "\n",
    "df_cross = pd.DataFrame(cross_rows)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Mean score comparison\n",
    "pivot_mean = df_cross.pivot(index=\"Dataset\", columns=\"Dimension\", values=\"Mean\")\n",
    "pivot_mean.plot(kind=\"bar\", ax=axes[0], colormap=\"Set2\", edgecolor=\"white\")\n",
    "axes[0].set_title(\"Mean GPT-4 Scores by Dataset and Dimension\")\n",
    "axes[0].set_ylabel(\"Mean Score\")\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=15)\n",
    "axes[0].legend(fontsize=8, loc=\"lower right\")\n",
    "\n",
    "# Score variance (std) comparison\n",
    "pivot_std = df_cross.pivot(index=\"Dataset\", columns=\"Dimension\", values=\"Std\")\n",
    "pivot_std.plot(kind=\"bar\", ax=axes[1], colormap=\"Set2\", edgecolor=\"white\")\n",
    "axes[1].set_title(\"GPT-4 Score Std Dev by Dataset and Dimension\")\n",
    "axes[1].set_ylabel(\"Standard Deviation\")\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=15)\n",
    "axes[1].legend(fontsize=8, loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness analysis: does GPT-4 tend to give high or low scores?\n",
    "pivot_skew = df_cross.pivot(index=\"Dataset\", columns=\"Dimension\", values=\"Skewness\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(pivot_skew, annot=True, fmt=\".2f\", cmap=\"RdBu_r\", center=0,\n",
    "            linewidths=0.5, square=True)\n",
    "plt.title(\"GPT-4 Score Skewness by Dataset and Dimension\\n\"\n",
    "          \"(Negative = tends high, Positive = tends low)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GPT-4 Evaluator Calibration & Bias\n",
    "\n",
    "### 6.1 Scale Usage Patterns\n",
    "Does GPT-4 use the full 1–5 scale, or does it cluster around certain values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze score frequency distribution per dataset\nn_datasets = len(gpt4_dialog)\ncols = min(3, n_datasets)\nrows_fig = (n_datasets + cols - 1) // cols\nfig, axes = plt.subplots(rows_fig, cols, figsize=(5 * cols, 4 * rows_fig))\nif rows_fig * cols > 1:\n    axes = axes.flatten()\nelse:\n    axes = [axes]\n\nfor idx, (ds_name, data) in enumerate(gpt4_dialog.items()):\n    if idx >= len(axes):\n        break\n    all_scores = []\n    for col in score_columns(data):          # <-- filter out non-score columns\n        all_scores.extend(data[col])\n    ax = axes[idx]\n    ax.hist(all_scores, bins=np.arange(0.5, 6.5, 1), color=\"steelblue\",\n            edgecolor=\"white\", density=True, alpha=0.8)\n    ax.set_title(f\"{ds_name} (n={len(all_scores)})\")\n    ax.set_xlabel(\"Score\")\n    ax.set_ylabel(\"Density\")\n    ax.set_xticks([1, 2, 3, 4, 5])\n\nfor i in range(len(gpt4_dialog), len(axes)):\n    axes[i].set_visible(False)\n\nplt.suptitle(\"GPT-4 Score Distribution by Dataset (All Dimensions Pooled)\",\n             fontsize=14, y=1.01)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quantify GPT-4's tendency toward extreme vs moderate scores\ncalibration_rows = []\nfor ds_name, data in gpt4_dialog.items():\n    all_scores = []\n    for col in score_columns(data):          # <-- filter out non-score columns\n        all_scores.extend(data[col])\n    all_scores = np.array(all_scores)\n    calibration_rows.append({\n        \"Dataset\": ds_name,\n        \"% Score=1\": np.mean(all_scores == 1) * 100,\n        \"% Score=2\": np.mean(all_scores == 2) * 100,\n        \"% Score=3\": np.mean(all_scores == 3) * 100,\n        \"% Score=4\": np.mean(all_scores == 4) * 100,\n        \"% Score=5\": np.mean(all_scores == 5) * 100,\n        \"% Extreme (1 or 5)\": np.mean((all_scores == 1) | (all_scores == 5)) * 100,\n        \"Mean\": np.mean(all_scores),\n    })\n\ndf_calib = pd.DataFrame(calibration_rows)\nprint(\"=== GPT-4 Scale Usage Patterns ===\")\nprint(df_calib.round(1).to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Positivity Bias Detection\n",
    "Does GPT-4 systematically rate dialogues higher than human annotators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GPT-4 vs human score distributions on overlapping FED dialogues\n",
    "bias_results = []\n",
    "for gpt4_dim, human_dim in dim_mapping.items():\n",
    "    rater_scores = [fed_gpt4[f\"{gpt4_dim}_{r}\"] for r in range(1, 6)\n",
    "                    if f\"{gpt4_dim}_{r}\" in fed_gpt4]\n",
    "    if not rater_scores:\n",
    "        continue\n",
    "    gpt4_mean_per_dialog = np.mean(rater_scores, axis=0)\n",
    "\n",
    "    human_mean_per_dialog = []\n",
    "    for item in fed_human:\n",
    "        if human_dim in item[\"annotations\"]:\n",
    "            human_mean_per_dialog.append(np.mean(item[\"annotations\"][human_dim]))\n",
    "        else:\n",
    "            human_mean_per_dialog.append(np.nan)\n",
    "\n",
    "    min_len = min(len(gpt4_mean_per_dialog), len(human_mean_per_dialog))\n",
    "    g = gpt4_mean_per_dialog[:min_len]\n",
    "    h = np.array(human_mean_per_dialog[:min_len])\n",
    "    mask = ~np.isnan(h)\n",
    "    g_clean, h_clean = g[mask], h[mask]\n",
    "\n",
    "    if len(g_clean) >= 5:\n",
    "        # Normalize both to 0-1 range for fair comparison\n",
    "        g_norm = (g_clean - g_clean.min()) / (g_clean.max() - g_clean.min() + 1e-8)\n",
    "        h_norm = (h_clean - h_clean.min()) / (h_clean.max() - h_clean.min() + 1e-8)\n",
    "        bias = np.mean(g_norm - h_norm)  # positive = GPT-4 rates higher\n",
    "\n",
    "        bias_results.append({\n",
    "            \"Dimension\": dimensions[gpt4_dim],\n",
    "            \"GPT-4 Mean (raw)\": np.mean(g_clean),\n",
    "            \"Human Mean (raw)\": np.mean(h_clean),\n",
    "            \"Normalized Bias\": bias,\n",
    "            \"Direction\": \"GPT-4 higher\" if bias > 0 else \"Human higher\",\n",
    "        })\n",
    "\n",
    "df_bias = pd.DataFrame(bias_results)\n",
    "print(\"=== Positivity Bias Analysis ===\")\n",
    "print(df_bias.round(3).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "colors_bar = [\"coral\" if b > 0 else \"steelblue\" for b in df_bias[\"Normalized Bias\"]]\n",
    "plt.barh(df_bias[\"Dimension\"], df_bias[\"Normalized Bias\"],\n",
    "         color=colors_bar, edgecolor=\"white\")\n",
    "plt.axvline(x=0, color=\"black\", linewidth=0.8)\n",
    "plt.xlabel(\"Normalized Bias (positive = GPT-4 rates higher)\")\n",
    "plt.title(\"GPT-4 Positivity Bias by Dimension (FED)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Per-Model Evaluation Bias\n",
    "Does GPT-4 systematically prefer certain dialogue models over others, compared to humans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group FED dialogues by model and compare GPT-4 vs Human Overall scores\n",
    "model_comparison = {}\n",
    "for i, item in enumerate(fed_human):\n",
    "    model = item.get(\"model\", \"unknown\")\n",
    "    if model not in model_comparison:\n",
    "        model_comparison[model] = {\"human\": [], \"gpt4\": []}\n",
    "\n",
    "    # Human Overall\n",
    "    if \"Overall\" in item[\"annotations\"]:\n",
    "        model_comparison[model][\"human\"].append(\n",
    "            np.mean(item[\"annotations\"][\"Overall\"]))\n",
    "    else:\n",
    "        model_comparison[model][\"human\"].append(np.nan)\n",
    "\n",
    "    # GPT-4 Overall (mean of 5 raters)\n",
    "    gpt4_scores_i = []\n",
    "    for r in range(1, 6):\n",
    "        col = f\"ovr_{r}\"\n",
    "        if col in fed_gpt4 and i < len(fed_gpt4[col]):\n",
    "            gpt4_scores_i.append(fed_gpt4[col][i])\n",
    "    if gpt4_scores_i:\n",
    "        model_comparison[model][\"gpt4\"].append(np.mean(gpt4_scores_i))\n",
    "    else:\n",
    "        model_comparison[model][\"gpt4\"].append(np.nan)\n",
    "\n",
    "model_rows = []\n",
    "for model, scores in model_comparison.items():\n",
    "    h = [s for s in scores[\"human\"] if not np.isnan(s)]\n",
    "    g = [s for s in scores[\"gpt4\"] if not np.isnan(s)]\n",
    "    if h and g:\n",
    "        model_rows.append({\n",
    "            \"Model\": model,\n",
    "            \"Human Mean\": np.mean(h),\n",
    "            \"GPT-4 Mean\": np.mean(g),\n",
    "            \"Rank (Human)\": 0,\n",
    "            \"Rank (GPT-4)\": 0,\n",
    "            \"N\": min(len(h), len(g)),\n",
    "        })\n",
    "\n",
    "df_model = pd.DataFrame(model_rows).sort_values(\"Human Mean\", ascending=False)\n",
    "df_model[\"Rank (Human)\"] = range(1, len(df_model) + 1)\n",
    "df_model = df_model.sort_values(\"GPT-4 Mean\", ascending=False)\n",
    "df_model[\"Rank (GPT-4)\"] = range(1, len(df_model) + 1)\n",
    "df_model = df_model.sort_values(\"Rank (Human)\")\n",
    "\n",
    "print(\"=== Per-Model Ranking Comparison ===\")\n",
    "print(df_model.round(3).to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = range(len(df_model))\n",
    "width = 0.35\n",
    "ax.bar([i - width/2 for i in x], df_model[\"Human Mean\"], width,\n",
    "       label=\"Human\", color=\"steelblue\")\n",
    "ax.bar([i + width/2 for i in x], df_model[\"GPT-4 Mean\"], width,\n",
    "       label=\"GPT-4\", color=\"coral\")\n",
    "ax.set_xticks(list(x))\n",
    "ax.set_xticklabels(df_model[\"Model\"], rotation=15)\n",
    "ax.set_ylabel(\"Mean Overall Rating\")\n",
    "ax.set_title(\"Human vs GPT-4 Overall Rating by Dialog Model\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Robustness Analysis\n",
    "\n",
    "A reliable evaluator should detect quality degradation when dialogues are perturbed.\n",
    "We analyze the robustness perturbation data to understand:\n",
    "- How many perturbation types exist per dimension\n",
    "- Dialogue structure changes under perturbation\n",
    "\n",
    "### 7.1 Robustness Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize robustness data structure\n",
    "print(\"=== Dialog-Level Robustness Perturbations ===\")\n",
    "dialog_robust_rows = []\n",
    "for dim, files in robustness_data[\"dialog\"].items():\n",
    "    dim_full = {\"coh\": \"Coherence\", \"div\": \"Diversity\",\n",
    "                \"eng\": \"Engagement\", \"inf\": \"Informativeness\"}.get(dim, dim)\n",
    "    for fname, lines in files.items():\n",
    "        perturb_type = fname.replace(\"fed-\", \"\").replace(\"decode-\", \"\").replace(\"_text\", \"\")\n",
    "        dialog_robust_rows.append({\n",
    "            \"Dimension\": dim_full,\n",
    "            \"Perturbation\": perturb_type,\n",
    "            \"N Dialogues\": len(lines),\n",
    "            \"Avg Turns\": np.mean([len(l.split(\"\\t\")) - 1 for l in lines]) if lines else 0,\n",
    "        })\n",
    "\n",
    "df_robust_dialog = pd.DataFrame(dialog_robust_rows)\n",
    "if len(df_robust_dialog) > 0:\n",
    "    print(df_robust_dialog.round(1).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Turn-Level Robustness Perturbations ===\")\n",
    "turn_robust_rows = []\n",
    "for dim, files in robustness_data[\"turn\"].items():\n",
    "    dim_full = {\"int\": \"Interesting\", \"rel\": \"Relevance\",\n",
    "                \"spe\": \"Specificity\", \"und\": \"Understandability\"}.get(dim, dim)\n",
    "    for fname, lines in files.items():\n",
    "        is_original = \"original\" in fname\n",
    "        turn_robust_rows.append({\n",
    "            \"Dimension\": dim_full,\n",
    "            \"File\": fname,\n",
    "            \"Type\": \"Original\" if is_original else \"Perturbed\",\n",
    "            \"N Items\": len(lines),\n",
    "        })\n",
    "\n",
    "df_robust_turn = pd.DataFrame(turn_robust_rows)\n",
    "if len(df_robust_turn) > 0:\n",
    "    print(df_robust_turn.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Dialog-Level Perturbation Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how perturbations change dialogue structure\n",
    "original_turns = [len(d) for d in fed_originals] if fed_originals else []\n",
    "\n",
    "struct_rows = []\n",
    "for dim, files in robustness_data[\"dialog\"].items():\n",
    "    for fname, lines in files.items():\n",
    "        turns_per_dialog = [len(l.split(\"\\t\")) - 1 for l in lines]\n",
    "        struct_rows.append({\n",
    "            \"Perturbation\": fname.replace(\"_text\", \"\"),\n",
    "            \"Dimension\": dim,\n",
    "            \"Mean Turns\": np.mean(turns_per_dialog),\n",
    "            \"Std Turns\": np.std(turns_per_dialog),\n",
    "            \"Min Turns\": np.min(turns_per_dialog),\n",
    "            \"Max Turns\": np.max(turns_per_dialog),\n",
    "        })\n",
    "\n",
    "if struct_rows:\n",
    "    df_struct = pd.DataFrame(struct_rows)\n",
    "\n",
    "    from matplotlib.patches import Patch\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    colors_dim = {\"coh\": \"steelblue\", \"div\": \"coral\",\n",
    "                  \"eng\": \"mediumseagreen\", \"inf\": \"orchid\"}\n",
    "    bar_colors = [colors_dim.get(row[\"Dimension\"], \"gray\")\n",
    "                  for _, row in df_struct.iterrows()]\n",
    "    plt.barh(range(len(df_struct)), df_struct[\"Mean Turns\"], color=bar_colors,\n",
    "             xerr=df_struct[\"Std Turns\"], capsize=3, edgecolor=\"white\")\n",
    "    plt.yticks(range(len(df_struct)), df_struct[\"Perturbation\"], fontsize=9)\n",
    "    plt.xlabel(\"Mean Number of Turns\")\n",
    "    plt.title(\"Dialogue Structure Under Perturbation\")\n",
    "\n",
    "    if original_turns:\n",
    "        plt.axvline(x=np.mean(original_turns), color=\"black\", linestyle=\"--\",\n",
    "                    alpha=0.6, label=f\"Original mean ({np.mean(original_turns):.1f})\")\n",
    "\n",
    "    dim_labels = {\"coh\": \"Coherence\", \"div\": \"Diversity\",\n",
    "                  \"eng\": \"Engagement\", \"inf\": \"Informativeness\"}\n",
    "    legend_elements = [Patch(facecolor=c, label=dim_labels[d])\n",
    "                       for d, c in colors_dim.items() if d in dim_labels]\n",
    "    plt.legend(handles=legend_elements, loc=\"lower right\", fontsize=9)\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No dialog-level robustness data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Turn-Level Perturbation: Original vs Perturbed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of original vs perturbed turns\n",
    "print(\"=== Turn-Level Perturbation Examples ===\")\n",
    "for dim, files in robustness_data[\"turn\"].items():\n",
    "    dim_full = {\"int\": \"Interesting\", \"rel\": \"Relevance\",\n",
    "                \"spe\": \"Specificity\", \"und\": \"Understandability\"}.get(dim, dim)\n",
    "    originals = {k: v for k, v in files.items() if \"original\" in k}\n",
    "    perturbeds = {k: v for k, v in files.items() if \"perturbed\" in k}\n",
    "\n",
    "    for orig_name, orig_lines in originals.items():\n",
    "        perturb_name = orig_name.replace(\"original\", \"perturbed\")\n",
    "        if perturb_name in perturbeds:\n",
    "            perturbed_lines = perturbeds[perturb_name]\n",
    "            print(f\"\\n--- {dim_full}: {orig_name.replace('_text', '')} ---\")\n",
    "            if orig_lines and perturbed_lines:\n",
    "                orig_parts = orig_lines[0].split(\"\\t\")\n",
    "                pert_parts = perturbed_lines[0].split(\"\\t\")\n",
    "                print(f\"  Original ({len(orig_lines)} items):\")\n",
    "                print(f\"    ID: {orig_parts[0]}\")\n",
    "                print(f\"    Last turn: {orig_parts[-1][:100]}\")\n",
    "                print(f\"  Perturbed ({len(perturbed_lines)} items):\")\n",
    "                print(f\"    ID: {pert_parts[0]}\")\n",
    "                print(f\"    Last turn: {pert_parts[-1][:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text-level changes between original and perturbed\n",
    "perturbation_stats = []\n",
    "for dim, files in robustness_data[\"turn\"].items():\n",
    "    dim_full = {\"int\": \"Interesting\", \"rel\": \"Relevance\",\n",
    "                \"spe\": \"Specificity\", \"und\": \"Understandability\"}.get(dim, dim)\n",
    "    originals = {k: v for k, v in files.items() if \"original\" in k}\n",
    "    perturbeds = {k: v for k, v in files.items() if \"perturbed\" in k}\n",
    "\n",
    "    for orig_name, orig_lines in originals.items():\n",
    "        perturb_name = orig_name.replace(\"original\", \"perturbed\")\n",
    "        if perturb_name in perturbeds:\n",
    "            perturbed_lines = perturbeds[perturb_name]\n",
    "            n = min(len(orig_lines), len(perturbed_lines))\n",
    "            orig_lens = [len(l) for l in orig_lines[:n]]\n",
    "            pert_lens = [len(l) for l in perturbed_lines[:n]]\n",
    "            perturbation_stats.append({\n",
    "                \"Dimension\": dim_full,\n",
    "                \"Type\": orig_name.replace(\"-original_text\", \"\").replace(\"_text\", \"\"),\n",
    "                \"N Pairs\": n,\n",
    "                \"Orig Mean Len\": np.mean(orig_lens),\n",
    "                \"Pert Mean Len\": np.mean(pert_lens),\n",
    "                \"Len Change %\": ((np.mean(pert_lens) - np.mean(orig_lens)) /\n",
    "                                 (np.mean(orig_lens) + 1e-8) * 100),\n",
    "            })\n",
    "\n",
    "if perturbation_stats:\n",
    "    df_perturb = pd.DataFrame(perturbation_stats)\n",
    "    print(\"=== Turn-Level Perturbation Statistics ===\")\n",
    "    print(df_perturb.round(1).to_string(index=False))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    colors_pct = [\"coral\" if x > 0 else \"steelblue\"\n",
    "                  for x in df_perturb[\"Len Change %\"]]\n",
    "    plt.barh(df_perturb[\"Dimension\"] + \" / \" + df_perturb[\"Type\"],\n",
    "             df_perturb[\"Len Change %\"], color=colors_pct, edgecolor=\"white\")\n",
    "    plt.axvline(x=0, color=\"black\", linewidth=0.8)\n",
    "    plt.xlabel(\"Length Change (%)\")\n",
    "    plt.title(\"Text Length Change: Original vs Perturbed Turns\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No original/perturbed pairs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Turn-Level Deep Analysis\n",
    "\n",
    "### 8.1 Cross-Dataset Rating Comparison (All Dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a comprehensive turn-level DataFrame\n",
    "turn_summary_rows = []\n",
    "for rating_name, data in turn_ratings.items():\n",
    "    dim_name = rating_name.replace(\"turn_\", \"\").capitalize()\n",
    "    for ds_name, scores in data.items():\n",
    "        clean = [s for s in scores\n",
    "                 if s is not None and not (isinstance(s, float) and np.isnan(s))]\n",
    "        if clean:\n",
    "            turn_summary_rows.append({\n",
    "                \"Dimension\": dim_name,\n",
    "                \"Dataset\": ds_name,\n",
    "                \"N\": len(clean),\n",
    "                \"Mean\": np.mean(clean),\n",
    "                \"Std\": np.std(clean),\n",
    "                \"Min\": np.min(clean),\n",
    "                \"Max\": np.max(clean),\n",
    "            })\n",
    "\n",
    "df_turn_all = pd.DataFrame(turn_summary_rows)\n",
    "\n",
    "# Pivot: Mean rating by Dataset x Dimension\n",
    "pivot_turn = df_turn_all.pivot_table(index=\"Dataset\", columns=\"Dimension\",\n",
    "                                      values=\"Mean\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pivot_turn, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=0.5)\n",
    "plt.title(\"Turn-Level Mean Rating by Dataset and Dimension\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDatasets with highest Overall mean:\")\n",
    "if \"Overall\" in pivot_turn.columns:\n",
    "    print(pivot_turn[\"Overall\"].sort_values(ascending=False).round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Turn-Level Dimension Correlations (Per Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataset with multiple dimension ratings, compute correlation matrix\n",
    "dataset_dims = {}\n",
    "for rating_name, data in turn_ratings.items():\n",
    "    dim_name = rating_name.replace(\"turn_\", \"\").capitalize()\n",
    "    if dim_name == \"Ratings\":  # skip the combined file\n",
    "        continue\n",
    "    for ds_name, scores in data.items():\n",
    "        if ds_name not in dataset_dims:\n",
    "            dataset_dims[ds_name] = {}\n",
    "        dataset_dims[ds_name][dim_name] = scores\n",
    "\n",
    "# Plot correlation heatmaps for datasets with 3+ dimensions\n",
    "multi_dim_datasets = {k: v for k, v in dataset_dims.items() if len(v) >= 3}\n",
    "n_plots = len(multi_dim_datasets)\n",
    "\n",
    "if n_plots > 0:\n",
    "    cols = min(3, n_plots)\n",
    "    rows_fig = (n_plots + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows_fig, cols, figsize=(6 * cols, 5 * rows_fig))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    elif rows_fig * cols > 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, (ds_name, dim_data) in enumerate(multi_dim_datasets.items()):\n",
    "        min_len = min(len(v) for v in dim_data.values())\n",
    "        df_ds = pd.DataFrame({k: v[:min_len] for k, v in dim_data.items()})\n",
    "        df_ds = df_ds.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "\n",
    "        if len(df_ds) > 5 and idx < len(axes):\n",
    "            corr = df_ds.corr(method=\"spearman\")\n",
    "            sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\",\n",
    "                        vmin=-1, vmax=1, square=True, linewidths=0.5,\n",
    "                        ax=axes[idx])\n",
    "            axes[idx].set_title(f\"{ds_name} (n={len(df_ds)})\")\n",
    "\n",
    "    for i in range(n_plots, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.suptitle(\"Turn-Level Dimension Correlations (Spearman) by Dataset\",\n",
    "                 fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough multi-dimension datasets for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Rating Scale Heterogeneity Across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how different datasets use different rating scales\n",
    "scale_rows = []\n",
    "for ds_name, dim_data in dataset_dims.items():\n",
    "    all_scores = []\n",
    "    for scores in dim_data.values():\n",
    "        all_scores.extend([s for s in scores if s is not None])\n",
    "    if all_scores:\n",
    "        all_scores = np.array(all_scores, dtype=float)\n",
    "        all_scores = all_scores[~np.isnan(all_scores)]\n",
    "        if len(all_scores) > 0:\n",
    "            scale_rows.append({\n",
    "                \"Dataset\": ds_name,\n",
    "                \"Min\": np.min(all_scores),\n",
    "                \"Max\": np.max(all_scores),\n",
    "                \"Range\": np.max(all_scores) - np.min(all_scores),\n",
    "                \"Unique Values\": len(np.unique(all_scores)),\n",
    "                \"Mean\": np.mean(all_scores),\n",
    "                \"N Dimensions\": len(dim_data),\n",
    "            })\n",
    "\n",
    "df_scale = pd.DataFrame(scale_rows).sort_values(\"Range\", ascending=False)\n",
    "print(\"=== Rating Scale Summary by Source Dataset ===\")\n",
    "print(df_scale.round(2).to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for _, row in df_scale.iterrows():\n",
    "    ax.barh(row[\"Dataset\"], row[\"Range\"], left=row[\"Min\"],\n",
    "            color=\"steelblue\", alpha=0.7, edgecolor=\"white\")\n",
    "    ax.plot(row[\"Mean\"], row[\"Dataset\"], \"D\", color=\"coral\", markersize=8)\n",
    "ax.set_xlabel(\"Score\")\n",
    "ax.set_title(\"Rating Scale Range by Dataset (diamond = mean)\")\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Critic Agent Evaluation Framework\n",
    "\n",
    "Based on the benchmark data, we build a scoring framework that quantifies\n",
    "how well an LLM evaluator (\"critic agent\") performs across key criteria.\n",
    "\n",
    "### Evaluation Criteria\n",
    "\n",
    "| Criterion | Metric | Weight | Source |\n",
    "|-----------|--------|--------|--------|\n",
    "| **Alignment** | Spearman correlation with human ratings | 0.30 | Section 5.2 |\n",
    "| **Consistency** | Inter-rater agreement (1 − mean std) | 0.20 | Section 5.1 |\n",
    "| **Discrimination** | Score variance (ability to differentiate) | 0.15 | Section 5.3 |\n",
    "| **Calibration** | Scale usage uniformity | 0.15 | Section 6.1 |\n",
    "| **Generalization** | Cross-dataset score stability | 0.10 | Section 5.3 |\n",
    "| **Bias** | Absolute normalized bias | 0.10 | Section 6.2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute the Critic Agent Evaluation Scorecard for GPT-4\n\ndef compute_critic_scores(gpt4_data, human_data, dim_map, dims, bias_df):\n    \"\"\"Compute multi-dimensional critic agent evaluation scores.\"\"\"\n    scores = {}\n\n    # --- 1. Alignment (Spearman with human) ---\n    alignment_scores = []\n    for gpt4_dim, human_dim in dim_map.items():\n        rater_scores = [gpt4_data[f\"{gpt4_dim}_{r}\"] for r in range(1, 6)\n                        if f\"{gpt4_dim}_{r}\" in gpt4_data]\n        if not rater_scores:\n            continue\n        gpt4_mean = np.mean(rater_scores, axis=0)\n        human_mean = [\n            np.mean(item[\"annotations\"][human_dim])\n            if human_dim in item[\"annotations\"] else np.nan\n            for item in human_data\n        ]\n        min_len = min(len(gpt4_mean), len(human_mean))\n        g, h = gpt4_mean[:min_len], np.array(human_mean[:min_len])\n        mask = ~np.isnan(h)\n        if np.sum(mask) >= 5:\n            rho, _ = stats.spearmanr(h[mask], g[mask])\n            alignment_scores.append(max(rho, 0))\n    scores[\"Alignment\"] = np.mean(alignment_scores) if alignment_scores else 0\n\n    # --- 2. Consistency (1 - normalized mean std across raters) ---\n    consistency_scores = []\n    for dim_key in dims:\n        rater_cols = [f\"{dim_key}_{r}\" for r in range(1, 6)\n                      if f\"{dim_key}_{r}\" in gpt4_data]\n        if len(rater_cols) < 2:\n            continue\n        rater_matrix = np.array([gpt4_data[c] for c in rater_cols])\n        mean_std = np.mean(np.std(rater_matrix, axis=0))\n        consistency_scores.append(1 - min(mean_std / 2.0, 1.0))\n    scores[\"Consistency\"] = (np.mean(consistency_scores)\n                             if consistency_scores else 0)\n\n    # --- 3. Discrimination (normalized std of mean scores) ---\n    disc_scores = []\n    for dim_key in dims:\n        rater_cols = [f\"{dim_key}_{r}\" for r in range(1, 6)\n                      if f\"{dim_key}_{r}\" in gpt4_data]\n        if not rater_cols:\n            continue\n        mean_per_dialog = np.mean([gpt4_data[c] for c in rater_cols], axis=0)\n        disc_scores.append(min(np.std(mean_per_dialog) / 2.0, 1.0))\n    scores[\"Discrimination\"] = np.mean(disc_scores) if disc_scores else 0\n\n    # --- 4. Calibration (scale usage uniformity via entropy) ---\n    all_scores_flat = []\n    for col in score_columns(gpt4_data):     # <-- filter out non-score columns\n        all_scores_flat.extend(gpt4_data[col])\n    all_scores_flat = np.array(all_scores_flat)\n    counts = np.array([np.sum(all_scores_flat == s) for s in [1, 2, 3, 4, 5]])\n    counts = counts / (counts.sum() + 1e-8)\n    entropy = -np.sum(counts * np.log(counts + 1e-8))\n    max_entropy = np.log(5)\n    scores[\"Calibration\"] = entropy / max_entropy\n\n    # --- 5. Generalization (1 - CoV of means across dims) ---\n    dim_means = []\n    for dim_key in dims:\n        rater_cols = [f\"{dim_key}_{r}\" for r in range(1, 6)\n                      if f\"{dim_key}_{r}\" in gpt4_data]\n        if rater_cols:\n            dim_means.append(\n                np.mean([np.mean(gpt4_data[c]) for c in rater_cols]))\n    if dim_means and np.mean(dim_means) > 0:\n        cov = np.std(dim_means) / np.mean(dim_means)\n        scores[\"Generalization\"] = max(1 - cov, 0)\n    else:\n        scores[\"Generalization\"] = 0\n\n    # --- 6. Bias (1 - absolute bias) ---\n    if bias_df is not None and len(bias_df) > 0:\n        mean_abs_bias = np.mean(np.abs(bias_df[\"Normalized Bias\"]))\n        scores[\"Low Bias\"] = max(1 - mean_abs_bias * 2, 0)\n    else:\n        scores[\"Low Bias\"] = 0.5\n\n    return scores\n\n\ncritic_scores = compute_critic_scores(\n    fed_gpt4, fed_human, dim_mapping, list(dimensions.keys()), df_bias\n)\n\n# Display scorecard\nweights = {\"Alignment\": 0.30, \"Consistency\": 0.20, \"Discrimination\": 0.15,\n           \"Calibration\": 0.15, \"Generalization\": 0.10, \"Low Bias\": 0.10}\n\nscorecard_rows = []\nfor criterion, score in critic_scores.items():\n    w = weights.get(criterion, 0)\n    scorecard_rows.append({\n        \"Criterion\": criterion,\n        \"Score (0-1)\": score,\n        \"Weight\": w,\n        \"Weighted\": score * w,\n    })\n\ndf_scorecard = pd.DataFrame(scorecard_rows)\noverall_score = df_scorecard[\"Weighted\"].sum()\n\nprint(\"=== GPT-4 Critic Agent Scorecard (FED Dataset) ===\")\nprint(df_scorecard.round(3).to_string(index=False))\nprint(f\"\\nOverall Critic Score: {overall_score:.3f} / 1.000\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart of critic agent scores\n",
    "labels = list(critic_scores.keys())\n",
    "values = list(critic_scores.values())\n",
    "values += values[:1]  # close the polygon\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "ax.fill(angles, values, alpha=0.2, color=\"steelblue\")\n",
    "ax.plot(angles, values, \"o-\", linewidth=2, color=\"steelblue\", markersize=8)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(labels, fontsize=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels([\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], fontsize=8)\n",
    "ax.set_title(f\"GPT-4 Critic Agent Profile\\n(Overall: {overall_score:.3f})\",\n",
    "             fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Multi-Dataset Critic Scorecard\n",
    "\n",
    "Compute the scorecard for GPT-4 across all available datasets (using\n",
    "consistency and discrimination metrics where human data is not available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute partial scorecards for each GPT-4 dataset\nmulti_ds_scores = []\nfor ds_name, data in gpt4_dialog.items():\n    ds_scores = {}\n\n    # Consistency\n    consistency_vals = []\n    for dim_key in dimensions:\n        rater_cols = [f\"{dim_key}_{r}\" for r in range(1, 6)\n                      if f\"{dim_key}_{r}\" in data]\n        if len(rater_cols) >= 2:\n            rater_matrix = np.array([data[c] for c in rater_cols])\n            mean_std = np.mean(np.std(rater_matrix, axis=0))\n            consistency_vals.append(1 - min(mean_std / 2.0, 1.0))\n    ds_scores[\"Consistency\"] = (np.mean(consistency_vals)\n                                if consistency_vals else 0)\n\n    # Discrimination\n    disc_vals = []\n    for dim_key in dimensions:\n        rater_cols = [f\"{dim_key}_{r}\" for r in range(1, 6)\n                      if f\"{dim_key}_{r}\" in data]\n        if rater_cols:\n            mean_per_dialog = np.mean([data[c] for c in rater_cols], axis=0)\n            disc_vals.append(min(np.std(mean_per_dialog) / 2.0, 1.0))\n    ds_scores[\"Discrimination\"] = np.mean(disc_vals) if disc_vals else 0\n\n    # Calibration (use only score columns, not \"indices\")\n    all_s = []\n    for col in score_columns(data):          # <-- filter out non-score columns\n        all_s.extend(data[col])\n    all_s = np.array(all_s)\n    counts = np.array([np.sum(all_s == s) for s in [1, 2, 3, 4, 5]])\n    counts = counts / (counts.sum() + 1e-8)\n    entropy = -np.sum(counts * np.log(counts + 1e-8))\n    ds_scores[\"Calibration\"] = entropy / np.log(5)\n\n    multi_ds_scores.append({\"Dataset\": ds_name, **ds_scores})\n\ndf_multi = pd.DataFrame(multi_ds_scores)\nprint(\"=== GPT-4 Critic Scores Across Datasets ===\")\nprint(df_multi.round(3).to_string(index=False))\n\ndf_multi_plot = df_multi.set_index(\"Dataset\")\ndf_multi_plot.plot(kind=\"bar\", figsize=(12, 5), colormap=\"Set2\", edgecolor=\"white\")\nplt.title(\"GPT-4 Critic Agent Scores Across Datasets\")\nplt.ylabel(\"Score (0-1)\")\nplt.xticks(rotation=15)\nplt.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Dimension-Level Deep Dive: What Makes a Good Evaluator?\n",
    "\n",
    "### 10.1 Per-Dimension Rater Correlation Matrix (FED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build rater-level correlation matrices for each dimension\n",
    "fig, axes = plt.subplots(1, 5, figsize=(24, 4))\n",
    "\n",
    "for idx, (dim_key, dim_label) in enumerate(dimensions.items()):\n",
    "    rater_cols = [f\"{dim_key}_{r}\" for r in range(1, 6)\n",
    "                  if f\"{dim_key}_{r}\" in fed_gpt4]\n",
    "    if len(rater_cols) < 2:\n",
    "        continue\n",
    "    df_raters = pd.DataFrame({\n",
    "        f\"R{r}\": fed_gpt4[f\"{dim_key}_{r}\"]\n",
    "        for r in range(1, 6) if f\"{dim_key}_{r}\" in fed_gpt4\n",
    "    })\n",
    "    corr = df_raters.corr(method=\"spearman\")\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"YlOrRd\", vmin=0, vmax=1,\n",
    "                square=True, linewidths=0.5, ax=axes[idx], cbar=(idx == 4))\n",
    "    axes[idx].set_title(f\"{dim_label}\")\n",
    "\n",
    "plt.suptitle(\"GPT-4 Pairwise Rater Correlation (Spearman) by Dimension\",\n",
    "             fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Score Stability: Coefficient of Variation per Dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dialogue, compute the Coefficient of Variation across 5 raters\n",
    "cv_data = []\n",
    "for dim_key, dim_label in dimensions.items():\n",
    "    rater_cols = [f\"{dim_key}_{r}\" for r in range(1, 6)\n",
    "                  if f\"{dim_key}_{r}\" in fed_gpt4]\n",
    "    if len(rater_cols) < 2:\n",
    "        continue\n",
    "    rater_matrix = np.array([fed_gpt4[c] for c in rater_cols])  # (5, N)\n",
    "    means = np.mean(rater_matrix, axis=0)\n",
    "    stds = np.std(rater_matrix, axis=0)\n",
    "    cvs = stds / (means + 1e-8)\n",
    "    for cv_val, mean_val in zip(cvs, means):\n",
    "        cv_data.append({\"Dimension\": dim_label, \"CV\": cv_val,\n",
    "                        \"Mean Score\": mean_val})\n",
    "\n",
    "df_cv = pd.DataFrame(cv_data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# CV distribution by dimension\n",
    "sns.boxplot(data=df_cv, x=\"Dimension\", y=\"CV\", hue=\"Dimension\",\n",
    "            palette=\"Set2\", legend=False, ax=axes[0])\n",
    "axes[0].set_title(\"Score Instability (CV) by Dimension\")\n",
    "axes[0].set_ylabel(\"Coefficient of Variation\")\n",
    "\n",
    "# CV vs Mean Score scatter\n",
    "for dim_label in dimensions.values():\n",
    "    subset = df_cv[df_cv[\"Dimension\"] == dim_label]\n",
    "    axes[1].scatter(subset[\"Mean Score\"], subset[\"CV\"],\n",
    "                    alpha=0.3, s=20, label=dim_label)\n",
    "axes[1].set_xlabel(\"Mean GPT-4 Score\")\n",
    "axes[1].set_ylabel(\"Coefficient of Variation\")\n",
    "axes[1].set_title(\"Score Instability vs Score Level\")\n",
    "axes[1].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean CV by dimension (lower = more consistent):\")\n",
    "print(df_cv.groupby(\"Dimension\")[\"CV\"].agg(\n",
    "    [\"mean\", \"median\", \"max\"]).round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EVALUATOR AGENT BENCHMARK - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n[Data Scope]\")\n",
    "print(f\"  Dialog-level GPT-4 annotation sets: {len(gpt4_dialog)}\")\n",
    "for name, data in gpt4_dialog.items():\n",
    "    n = len(list(data.values())[0])\n",
    "    print(f\"    {name}: {n} dialogues, {len(data)} columns\")\n",
    "print(f\"  Dialog-level human annotations (FED): {len(fed_human)} dialogues\")\n",
    "print(f\"  Turn-level rating files: {len(turn_ratings)}\")\n",
    "print(f\"  Robustness perturbation types: \"\n",
    "      f\"{sum(len(v) for v in robustness_data['dialog'].values())} dialog-level, \"\n",
    "      f\"{sum(len(v) for v in robustness_data['turn'].values())} turn-level\")\n",
    "\n",
    "print(f\"\\n[GPT-4 vs Human Alignment (FED)]\")\n",
    "if len(df_corr) > 0:\n",
    "    for _, row in df_corr.iterrows():\n",
    "        sig = (\"***\" if row[\"Spearman p\"] < 0.001 else\n",
    "               \"**\" if row[\"Spearman p\"] < 0.01 else\n",
    "               \"*\" if row[\"Spearman p\"] < 0.05 else \"ns\")\n",
    "        print(f\"  {row['GPT-4 Dim']:18s}: Spearman={row['Spearman r']:.3f}{sig}, \"\n",
    "              f\"Pearson={row['Pearson r']:.3f}\")\n",
    "\n",
    "print(f\"\\n[Critic Agent Overall Score]: {overall_score:.3f} / 1.000\")\n",
    "for _, row in df_scorecard.iterrows():\n",
    "    bar = '#' * int(row['Score (0-1)'] * 20)\n",
    "    print(f\"  {row['Criterion']:18s}: {row['Score (0-1)']:.3f} [{bar:<20s}]\")\n",
    "\n",
    "print(f\"\\n[Key Takeaways]\")\n",
    "print(f\"  1. GPT-4 shows moderate consistency (mean CV varies by dimension)\")\n",
    "print(f\"  2. Human-LLM alignment is dimension-dependent\")\n",
    "print(f\"  3. Score distributions reveal calibration patterns (scale usage bias)\")\n",
    "print(f\"  4. Robustness data enables perturbation-sensitivity testing\")\n",
    "print(f\"  5. The critic scoring framework provides a multi-criteria evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Observations\n",
    "\n",
    "1. **Dimension-dependent alignment:** GPT-4's correlation with human ratings\n",
    "   varies significantly across quality dimensions. Some dimensions (e.g., Coherence)\n",
    "   are easier for LLMs to evaluate than others (e.g., Engagement/Likeability).\n",
    "\n",
    "2. **Systematic scoring biases:** GPT-4 exhibits measurable positivity or negativity\n",
    "   bias depending on the dimension, with a tendency to compress the score range\n",
    "   compared to human annotators.\n",
    "\n",
    "3. **Intra-model variance is non-trivial:** Even running the same GPT-4 model\n",
    "   5 times produces different scores, with Coefficient of Variation highest\n",
    "   for low-scoring dialogues (uncertain evaluations).\n",
    "\n",
    "4. **Cross-dataset generalization gaps:** GPT-4's evaluator quality metrics\n",
    "   (consistency, discrimination, calibration) shift substantially across\n",
    "   source datasets, suggesting evaluator performance is task-dependent.\n",
    "\n",
    "5. **Robustness as a critical dimension:** The robustness perturbation data\n",
    "   (order shuffling, repetition, contradiction) provides a way to test whether\n",
    "   evaluators detect quality degradation — essential for reliable critic agents.\n",
    "\n",
    "6. **Multi-criteria scoring framework:** The critic agent scorecard (Alignment,\n",
    "   Consistency, Discrimination, Calibration, Generalization, Bias) provides a\n",
    "   holistic view of evaluator quality beyond simple correlation.\n",
    "\n",
    "7. **Research relevance (IS/AI):**\n",
    "   - **Critic agents:** Quantify LLM evaluator reliability for agent-based systems\n",
    "   - **Evaluation automation:** Identify which quality dimensions can be reliably automated\n",
    "   - **Calibration methods:** Design post-hoc calibration to reduce GPT-4 scoring bias\n",
    "   - **Ensemble evaluation:** Combine multiple LLM runs to reduce variance\n",
    "   - **Robustness testing:** Validate evaluator sensitivity to quality perturbations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}