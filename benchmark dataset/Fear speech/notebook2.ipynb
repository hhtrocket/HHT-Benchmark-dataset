{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fear Speech on Social Media (Gab)\n",
    "**Category:** AI Safety & Alignment\n",
    "\n",
    "**Source:** [OSF \u2013 On the rise of fear speech in online social media](https://osf.io/dc7vu/?view_only=8144833546e54a399ab883f0b0e3e7f7) |\n",
    "[GitHub](https://github.com/punyajoy/Fearspeech-project)\n",
    "\n",
    "**Description:** Designed for AI Safety Alignment, specifically to detect\n",
    "*implicit fear speech* \u2014 language that is not overtly profane but is\n",
    "maliciously inflammatory, instilling existential fear about a target\n",
    "community. Unlike hate speech which hurls direct insults, fear speech\n",
    "portrays a community as a perpetrator through a fabricated chain of\n",
    "argumentation.\n",
    "\n",
    "**Data Content:** 9,441 posts from the Gab platform, each annotated by\n",
    "3 crowd-workers as *fear speech*, *hate speech*, or *normal*. The\n",
    "extended dataset adds 28-dimensional emotion scores and per-token\n",
    "rationale (attention) weights.\n",
    "\n",
    "**Paper:** [On the rise of fear speech in online social media (PNAS 2023)](https://www.pnas.org/doi/10.1073/pnas.2212270120)\n",
    "\n",
    "---\n",
    "This notebook covers:\n",
    "1. Setup\n",
    "2. Dataset Overview\n",
    "3. Data Loading\n",
    "4. Schema & Samples\n",
    "5. Exploratory Data Analysis\n",
    "   - 5.1 Label Distribution\n",
    "   - 5.2 Text Length by Label\n",
    "   - 5.3 Target Community Analysis\n",
    "   - 5.4 Emotion Profiles by Label\n",
    "   - 5.5 Rationale (Attention) Analysis\n",
    "   - 5.6 Inter-Annotator Agreement\n",
    "   - 5.7 Train / Val / Test Split\n",
    "6. Classification Evaluation Framework\n",
    "7. Summary\n",
    "8. Key Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"axes.titlesize\"] = 13\n",
    "plt.rcParams[\"axes.labelsize\"] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview\n",
    "\n",
    "The OSF repository provides **three files**:\n",
    "\n",
    "| File | Size | Description |\n",
    "|------|------|-------------|\n",
    "| `final_dataset.json` | 11 MB | 9,441 posts with text, 3 annotations each, majority label |\n",
    "| `final_dataset_emotion_rationale.json` | 37 MB | Same posts + 28-dim emotion scores + per-token rationale weights |\n",
    "| `dataset_split.json` | 305 KB | Pre-defined train / val / test split (post IDs) |\n",
    "\n",
    "**Annotation scheme:**\n",
    "- Each post was labeled by **3 annotators** (AMT crowd-workers + experts)\n",
    "- Labels: `fearspeech`, `hatespeech`, `normal` (multi-label allowed)\n",
    "- Annotators also identified **target communities** (e.g., Islam, Refugee, African)\n",
    "- The `majority_label` field aggregates the 3 individual annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main dataset\n",
    "with open(\"final_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Load extended dataset with emotion & rationale\n",
    "with open(\"final_dataset_emotion_rationale.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_emo = json.load(f)\n",
    "\n",
    "# Load train/val/test split\n",
    "with open(\"dataset_split.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    split_ids = json.load(f)\n",
    "\n",
    "print(f\"Main dataset:     {len(raw_data):,} posts\")\n",
    "print(f\"Emotion dataset:  {len(raw_emo):,} posts\")\n",
    "print(f\"Split \u2014 train: {len(split_ids['train']):,}, \"\n",
    "      f\"val: {len(split_ids['val']):,}, \"\n",
    "      f\"test: {len(split_ids['test']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a flat DataFrame from the JSON\n",
    "rows = []\n",
    "for post_id, entry in raw_data.items():\n",
    "    labels = entry[\"majority_label\"]\n",
    "    is_fear = int(\"fearspeech\" in labels)\n",
    "    is_hate = int(\"hatespeech\" in labels)\n",
    "    is_normal = int(\"normal\" in labels)\n",
    "\n",
    "    # Collect all annotator targets\n",
    "    all_targets = []\n",
    "    for ann in entry[\"annotations\"]:\n",
    "        all_targets.extend([t for t in ann[\"Targets\"] if t != \"None\"])\n",
    "\n",
    "    # Determine primary label (single-label view)\n",
    "    if is_fear and is_hate:\n",
    "        primary = \"fear+hate\"\n",
    "    elif is_fear:\n",
    "        primary = \"fearspeech\"\n",
    "    elif is_hate:\n",
    "        primary = \"hatespeech\"\n",
    "    elif is_normal:\n",
    "        primary = \"normal\"\n",
    "    else:\n",
    "        primary = \"none\"\n",
    "\n",
    "    rows.append({\n",
    "        \"post_id\": post_id,\n",
    "        \"text\": entry[\"text\"],\n",
    "        \"is_fear\": is_fear,\n",
    "        \"is_hate\": is_hate,\n",
    "        \"is_normal\": is_normal,\n",
    "        \"primary_label\": primary,\n",
    "        \"n_labels\": is_fear + is_hate + is_normal,\n",
    "        \"targets\": list(set(all_targets)),\n",
    "        \"n_targets\": len(set(all_targets)),\n",
    "        \"word_count\": len(entry[\"text\"].split()),\n",
    "        \"char_count\": len(entry[\"text\"]),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Schema & Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column overview\n",
    "print(\"=== DataFrame Columns ===\")\n",
    "for col in df.columns:\n",
    "    dtype = df[col].dtype\n",
    "    if dtype == \"object\" and isinstance(df[col].iloc[0], list):\n",
    "        info = \"(list column)\"\n",
    "    else:\n",
    "        info = df[col].nunique()\n",
    "    print(f\"  {col:20s}  dtype={str(dtype):10s}  unique={info}\")\n",
    "\n",
    "print(f\"\n=== Raw JSON fields per post ===\")\n",
    "sample_key = list(raw_data.keys())[0]\n",
    "print(f\"  Keys: {list(raw_data[sample_key].keys())}\")\n",
    "print(f\"  Extended keys: {list(raw_emo[sample_key].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample posts from each category\n",
    "for label in [\"fearspeech\", \"hatespeech\", \"normal\", \"fear+hate\"]:\n",
    "    subset = df[df[\"primary_label\"] == label]\n",
    "    if len(subset) == 0:\n",
    "        continue\n",
    "    row = subset.iloc[0]\n",
    "    text_preview = row[\"text\"][:200] + (\"...\" if len(row[\"text\"]) > 200 else \"\")\n",
    "    print(f\"[{label.upper()}]\")\n",
    "    print(f\"  Text: {text_preview}\")\n",
    "    print(f\"  Targets: {row['targets']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "### 5.1 Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-label view\n",
    "primary_counts = df[\"primary_label\"].value_counts()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "colors_map = {\n",
    "    \"normal\": \"steelblue\",\n",
    "    \"hatespeech\": \"coral\",\n",
    "    \"fearspeech\": \"orchid\",\n",
    "    \"fear+hate\": \"goldenrod\",\n",
    "    \"none\": \"lightgray\",\n",
    "}\n",
    "bar_colors = [colors_map.get(lab, \"gray\") for lab in primary_counts.index]\n",
    "bars = axes[0].bar(primary_counts.index, primary_counts.values,\n",
    "                   color=bar_colors, edgecolor=\"white\")\n",
    "axes[0].set_title(\"Post Count by Primary Label\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "for bar, val in zip(bars, primary_counts.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 50,\n",
    "                f\"{val:,}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "# Multi-label overlap: binary counts\n",
    "binary_counts = pd.Series({\n",
    "    \"fear\": df[\"is_fear\"].sum(),\n",
    "    \"hate\": df[\"is_hate\"].sum(),\n",
    "    \"normal\": df[\"is_normal\"].sum(),\n",
    "})\n",
    "axes[1].bar(binary_counts.index, binary_counts.values,\n",
    "            color=[\"orchid\", \"coral\", \"steelblue\"], edgecolor=\"white\")\n",
    "axes[1].set_title(\"Posts Containing Each Label (multi-label)\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "for i, val in enumerate(binary_counts.values):\n",
    "    axes[1].text(i, val + 50, f\"{val:,}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Multi-label posts (fear+hate): {(df['primary_label']=='fear+hate').sum():,}\")\n",
    "print(f\"Posts with label 'none': {(df['primary_label']=='none').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Text Length by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "label_order = [\"normal\", \"hatespeech\", \"fearspeech\", \"fear+hate\"]\n",
    "palette = [colors_map[l] for l in label_order]\n",
    "plot_df = df[df[\"primary_label\"].isin(label_order)]\n",
    "\n",
    "# Word count boxplot\n",
    "sns.boxplot(data=plot_df, x=\"primary_label\", y=\"word_count\",\n",
    "            order=label_order, palette=palette, ax=axes[0],\n",
    "            showfliers=False)\n",
    "axes[0].set_title(\"Word Count by Label\")\n",
    "axes[0].set_xlabel(\"Label\")\n",
    "axes[0].set_ylabel(\"Word Count\")\n",
    "\n",
    "# Overlapping histograms\n",
    "for label, color in zip(label_order[:3], palette[:3]):\n",
    "    subset = df[df[\"primary_label\"] == label][\"word_count\"]\n",
    "    axes[1].hist(subset, bins=40, alpha=0.5, color=color,\n",
    "                label=label, edgecolor=\"white\")\n",
    "axes[1].set_title(\"Word Count Distribution\")\n",
    "axes[1].set_xlabel(\"Word Count\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics table\n",
    "stats = (plot_df.groupby(\"primary_label\")[\"word_count\"]\n",
    "         .describe()\n",
    "         .round(1)\n",
    "         .loc[label_order])\n",
    "print(stats[[\"count\", \"mean\", \"50%\", \"min\", \"max\"]]\n",
    "      .rename(columns={\"50%\": \"median\"})\n",
    "      .to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Target Community Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count targets across all annotations\n",
    "target_counter = Counter()\n",
    "fear_targets = Counter()\n",
    "hate_targets = Counter()\n",
    "\n",
    "for post_id, entry in raw_data.items():\n",
    "    labels = entry[\"majority_label\"]\n",
    "    for ann in entry[\"annotations\"]:\n",
    "        for t in ann[\"Targets\"]:\n",
    "            if t == \"None\":\n",
    "                continue\n",
    "            target_counter[t] += 1\n",
    "            if \"fearspeech\" in labels:\n",
    "                fear_targets[t] += 1\n",
    "            if \"hatespeech\" in labels:\n",
    "                hate_targets[t] += 1\n",
    "\n",
    "top_targets = [t for t, _ in target_counter.most_common(12)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Overall target distribution\n",
    "top_vals = [target_counter[t] for t in top_targets]\n",
    "axes[0].barh(top_targets[::-1], top_vals[::-1], color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[0].set_title(\"Top 12 Target Communities (all annotations)\")\n",
    "axes[0].set_xlabel(\"Mention Count\")\n",
    "\n",
    "# Fear vs Hate target comparison\n",
    "x = np.arange(len(top_targets))\n",
    "width = 0.35\n",
    "fear_vals = [fear_targets.get(t, 0) for t in top_targets]\n",
    "hate_vals = [hate_targets.get(t, 0) for t in top_targets]\n",
    "axes[1].barh(x + width / 2, fear_vals, width, label=\"Fear Speech\",\n",
    "             color=\"orchid\", edgecolor=\"white\")\n",
    "axes[1].barh(x - width / 2, hate_vals, width, label=\"Hate Speech\",\n",
    "             color=\"coral\", edgecolor=\"white\")\n",
    "axes[1].set_yticks(x)\n",
    "axes[1].set_yticklabels(top_targets)\n",
    "axes[1].set_title(\"Target Communities: Fear vs Hate Speech\")\n",
    "axes[1].set_xlabel(\"Mention Count\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total unique targets: {len(target_counter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Emotion Profiles by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build emotion matrix\n",
    "emotion_rows = []\n",
    "for post_id, entry in raw_emo.items():\n",
    "    emo = {k: float(v) for k, v in entry[\"emotion_dict\"].items()}\n",
    "    labels = entry[\"majority_label\"]\n",
    "    if \"fearspeech\" in labels and \"hatespeech\" in labels:\n",
    "        primary = \"fear+hate\"\n",
    "    elif \"fearspeech\" in labels:\n",
    "        primary = \"fearspeech\"\n",
    "    elif \"hatespeech\" in labels:\n",
    "        primary = \"hatespeech\"\n",
    "    elif \"normal\" in labels:\n",
    "        primary = \"normal\"\n",
    "    else:\n",
    "        primary = \"none\"\n",
    "    emo[\"primary_label\"] = primary\n",
    "    emotion_rows.append(emo)\n",
    "\n",
    "df_emo = pd.DataFrame(emotion_rows)\n",
    "emotion_cols = [c for c in df_emo.columns if c != \"primary_label\"]\n",
    "\n",
    "# Mean emotion by label\n",
    "emo_by_label = (df_emo[df_emo[\"primary_label\"].isin(label_order)]\n",
    "                .groupby(\"primary_label\")[emotion_cols]\n",
    "                .mean()\n",
    "                .loc[label_order])\n",
    "\n",
    "# Heatmap of emotions\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "sns.heatmap(emo_by_label, cmap=\"YlOrRd\", annot=False, ax=ax,\n",
    "            linewidths=0.5, linecolor=\"white\")\n",
    "ax.set_title(\"Mean Emotion Scores by Label\")\n",
    "ax.set_ylabel(\"Label\")\n",
    "ax.set_xlabel(\"Emotion\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top discriminative emotions: fear speech vs normal\n",
    "fear_mean = emo_by_label.loc[\"fearspeech\"]\n",
    "normal_mean = emo_by_label.loc[\"normal\"]\n",
    "diff = (fear_mean - normal_mean).sort_values(ascending=False)\n",
    "\n",
    "# Exclude 'neutral' (dominant everywhere)\n",
    "diff_filtered = diff.drop(\"neutral\", errors=\"ignore\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "colors = [\"orchid\" if v > 0 else \"steelblue\" for v in diff_filtered.values]\n",
    "ax.barh(diff_filtered.index[::-1], diff_filtered.values[::-1],\n",
    "        color=colors[::-1], edgecolor=\"white\")\n",
    "ax.axvline(0, color=\"black\", linewidth=0.8)\n",
    "ax.set_title(\"Emotion Difference: Fear Speech vs Normal\")\n",
    "ax.set_xlabel(\"Mean Score Difference (positive = higher in fear speech)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 emotions elevated in fear speech:\")\n",
    "for emo_name, val in diff_filtered.head(5).items():\n",
    "    print(f\"  {emo_name}: +{val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Rationale (Attention) Analysis\n",
    "\n",
    "Each post has a `rationale_dict` mapping BERT tokens to importance scores.\n",
    "Higher scores indicate tokens the model considers more relevant for\n",
    "classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate token rationale scores across fear speech posts\n",
    "fear_token_scores = defaultdict(list)\n",
    "hate_token_scores = defaultdict(list)\n",
    "\n",
    "for post_id, entry in raw_emo.items():\n",
    "    labels = entry[\"majority_label\"]\n",
    "    rationale = entry[\"rationale_dict\"]\n",
    "    for token, score in rationale.items():\n",
    "        score_f = float(score)\n",
    "        if \"fearspeech\" in labels:\n",
    "            fear_token_scores[token].append(score_f)\n",
    "        if \"hatespeech\" in labels:\n",
    "            hate_token_scores[token].append(score_f)\n",
    "\n",
    "# Mean score per token (filter tokens with >= 10 occurrences)\n",
    "fear_token_mean = {t: np.mean(s) for t, s in fear_token_scores.items() if len(s) >= 10}\n",
    "hate_token_mean = {t: np.mean(s) for t, s in hate_token_scores.items() if len(s) >= 10}\n",
    "\n",
    "# Top fear speech tokens\n",
    "top_fear_tokens = sorted(fear_token_mean.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "top_hate_tokens = sorted(hate_token_mean.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "tokens_f, scores_f = zip(*top_fear_tokens)\n",
    "axes[0].barh(tokens_f[::-1], scores_f[::-1], color=\"orchid\", edgecolor=\"white\")\n",
    "axes[0].set_title(\"Top 20 Rationale Tokens (Fear Speech)\")\n",
    "axes[0].set_xlabel(\"Mean Rationale Score\")\n",
    "\n",
    "tokens_h, scores_h = zip(*top_hate_tokens)\n",
    "axes[1].barh(tokens_h[::-1], scores_h[::-1], color=\"coral\", edgecolor=\"white\")\n",
    "axes[1].set_title(\"Top 20 Rationale Tokens (Hate Speech)\")\n",
    "axes[1].set_xlabel(\"Mean Rationale Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Unique tokens in fear speech rationales: {len(fear_token_mean):,}\")\n",
    "print(f\"Unique tokens in hate speech rationales: {len(hate_token_mean):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Inter-Annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze annotator agreement\n",
    "agreement_levels = {\"full\": 0, \"partial\": 0, \"none\": 0}\n",
    "label_confusion = defaultdict(int)\n",
    "\n",
    "for entry in raw_data.values():\n",
    "    classes = [tuple(sorted(ann[\"Class\"])) for ann in entry[\"annotations\"]]\n",
    "    unique_classes = set(classes)\n",
    "    if len(unique_classes) == 1:\n",
    "        agreement_levels[\"full\"] += 1\n",
    "    elif len(unique_classes) == 2:\n",
    "        agreement_levels[\"partial\"] += 1\n",
    "    else:\n",
    "        agreement_levels[\"none\"] += 1\n",
    "\n",
    "    # Track which label pairs get confused\n",
    "    flat = [c for ann_class in classes for c in ann_class]\n",
    "    unique_flat = set(flat)\n",
    "    if len(unique_flat) > 1:\n",
    "        for a in unique_flat:\n",
    "            for b in unique_flat:\n",
    "                if a < b:\n",
    "                    label_confusion[(a, b)] += 1\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Agreement pie chart\n",
    "agree_labels = list(agreement_levels.keys())\n",
    "agree_vals = list(agreement_levels.values())\n",
    "agree_colors = [\"mediumseagreen\", \"goldenrod\", \"coral\"]\n",
    "axes[0].pie(agree_vals, labels=agree_labels, colors=agree_colors,\n",
    "            autopct=\"%1.1f%%\", startangle=90)\n",
    "axes[0].set_title(\"Inter-Annotator Agreement\")\n",
    "\n",
    "# Confusion between labels\n",
    "conf_sorted = sorted(label_confusion.items(), key=lambda x: x[1], reverse=True)\n",
    "pair_labels = [f\"{a} \\u2194 {b}\" for (a, b), _ in conf_sorted[:8]]\n",
    "pair_vals = [v for _, v in conf_sorted[:8]]\n",
    "axes[1].barh(pair_labels[::-1], pair_vals[::-1], color=\"goldenrod\", edgecolor=\"white\")\n",
    "axes[1].set_title(\"Most Confused Label Pairs\")\n",
    "axes[1].set_xlabel(\"Disagreement Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "total = sum(agreement_levels.values())\n",
    "print(f\"Full agreement:    {agreement_levels['full']:,} ({agreement_levels['full']/total*100:.1f}%)\")\n",
    "print(f\"Partial agreement: {agreement_levels['partial']:,} ({agreement_levels['partial']/total*100:.1f}%)\")\n",
    "print(f\"No agreement:      {agreement_levels['none']:,} ({agreement_levels['none']/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Train / Val / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign split labels\n",
    "id_to_split = {}\n",
    "for split_name, ids in split_ids.items():\n",
    "    for pid in ids:\n",
    "        id_to_split[pid] = split_name\n",
    "\n",
    "df[\"split\"] = df[\"post_id\"].map(id_to_split)\n",
    "\n",
    "# Label distribution per split\n",
    "split_dist = pd.crosstab(df[\"split\"], df[\"primary_label\"], margins=True)\n",
    "split_dist = split_dist.reindex([\"train\", \"val\", \"test\", \"All\"])\n",
    "print(\"=== Label Distribution per Split ===\")\n",
    "print(split_dist.to_string())\n",
    "\n",
    "# Percentage view\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "split_pct = pd.crosstab(df[\"split\"], df[\"primary_label\"], normalize=\"index\")\n",
    "split_pct = split_pct.reindex([\"train\", \"val\", \"test\"])\n",
    "col_order = [c for c in [\"normal\", \"hatespeech\", \"fearspeech\", \"fear+hate\", \"none\"]\n",
    "             if c in split_pct.columns]\n",
    "split_pct[col_order].plot(\n",
    "    kind=\"bar\", stacked=True, ax=ax,\n",
    "    color=[colors_map[c] for c in col_order], edgecolor=\"white\"\n",
    ")\n",
    "ax.set_title(\"Label Distribution per Split (%)\")\n",
    "ax.set_ylabel(\"Proportion\")\n",
    "ax.set_xlabel(\"Split\")\n",
    "ax.legend(title=\"Label\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classification Evaluation Framework\n",
    "\n",
    "We evaluate three simple baselines on the test split to establish lower\n",
    "bounds. The paper reports Gab-BERT achieving **macro-F1 = 0.62** on this\n",
    "task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare evaluation data\n",
    "test_df = df[df[\"split\"] == \"test\"].copy()\n",
    "\n",
    "# Map primary labels to numeric\n",
    "label_set = [\"normal\", \"hatespeech\", \"fearspeech\"]\n",
    "# Exclude fear+hate and none for clean 3-class eval\n",
    "test_3class = test_df[test_df[\"primary_label\"].isin(label_set)].copy()\n",
    "print(f\"Test set (3-class): {len(test_3class)} posts\")\n",
    "print(test_3class[\"primary_label\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "true_labels = test_3class[\"primary_label\"].values\n",
    "most_common = test_3class[\"primary_label\"].mode()[0]\n",
    "\n",
    "# --- Strategy 1: Random Baseline ---\n",
    "np.random.seed(42)\n",
    "random_preds = np.random.choice(label_set, size=len(true_labels))\n",
    "\n",
    "# --- Strategy 2: Majority Class ---\n",
    "majority_preds = np.full(len(true_labels), most_common)\n",
    "\n",
    "# --- Strategy 3: Keyword Heuristic ---\n",
    "fear_keywords = [\"threat\", \"invad\", \"replac\", \"danger\", \"destroy\", \"attack\",\n",
    "                 \"terror\", \"crime\", \"criminal\", \"rape\", \"murder\", \"kill\",\n",
    "                 \"flood\", \"invasion\", \"takeover\", \"conquer\"]\n",
    "hate_keywords = [\"stupid\", \"idiot\", \"moron\", \"trash\", \"scum\", \"loser\",\n",
    "                 \"ugly\", \"dumb\", \"pathetic\", \"disgust\", \"retard\",\n",
    "                 \"subhuman\", \"filth\", \"vermin\"]\n",
    "\n",
    "def keyword_predict(text):\n",
    "    text_lower = text.lower()\n",
    "    fear_score = sum(1 for kw in fear_keywords if kw in text_lower)\n",
    "    hate_score = sum(1 for kw in hate_keywords if kw in text_lower)\n",
    "    if fear_score > hate_score:\n",
    "        return \"fearspeech\"\n",
    "    elif hate_score > fear_score:\n",
    "        return \"hatespeech\"\n",
    "    elif fear_score > 0:\n",
    "        return \"fearspeech\"\n",
    "    return \"normal\"\n",
    "\n",
    "keyword_preds = test_3class[\"text\"].apply(keyword_predict).values\n",
    "\n",
    "# --- Evaluate all strategies ---\n",
    "results = []\n",
    "strategies = {\n",
    "    \"Random Baseline\": random_preds,\n",
    "    \"Majority Class\": majority_preds,\n",
    "    \"Keyword Heuristic\": keyword_preds,\n",
    "}\n",
    "\n",
    "for name, preds in strategies.items():\n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    macro_f1 = f1_score(true_labels, preds, labels=label_set, average=\"macro\")\n",
    "    per_class = f1_score(true_labels, preds, labels=label_set, average=None)\n",
    "    results.append({\n",
    "        \"Strategy\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Macro F1\": macro_f1,\n",
    "        \"F1 (normal)\": per_class[0],\n",
    "        \"F1 (hate)\": per_class[1],\n",
    "        \"F1 (fear)\": per_class[2],\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(results)\n",
    "print(df_eval.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize strategy comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart: Accuracy and Macro F1\n",
    "x = np.arange(len(df_eval))\n",
    "width = 0.35\n",
    "axes[0].bar(x - width / 2, df_eval[\"Accuracy\"], width,\n",
    "            label=\"Accuracy\", color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[0].bar(x + width / 2, df_eval[\"Macro F1\"], width,\n",
    "            label=\"Macro F1\", color=\"coral\", edgecolor=\"white\")\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(df_eval[\"Strategy\"], rotation=15, ha=\"right\")\n",
    "axes[0].set_title(\"Baseline Strategies: Accuracy & Macro F1\")\n",
    "axes[0].set_ylim(0, 0.7)\n",
    "axes[0].legend()\n",
    "# Reference line for Gab-BERT\n",
    "axes[0].axhline(0.62, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "axes[0].text(2.3, 0.63, \"Gab-BERT (F1=0.62)\", fontsize=9, color=\"gray\")\n",
    "\n",
    "# Per-class F1\n",
    "per_class_cols = [\"F1 (normal)\", \"F1 (hate)\", \"F1 (fear)\"]\n",
    "class_colors = [\"steelblue\", \"coral\", \"orchid\"]\n",
    "bar_width = 0.25\n",
    "for i, (col, color) in enumerate(zip(per_class_cols, class_colors)):\n",
    "    axes[1].bar(x + (i - 1) * bar_width, df_eval[col], bar_width,\n",
    "               label=col, color=color, edgecolor=\"white\")\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(df_eval[\"Strategy\"], rotation=15, ha=\"right\")\n",
    "axes[1].set_title(\"Per-Class F1 Scores\")\n",
    "axes[1].set_ylim(0, 0.7)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verify keyword heuristic beats random\n",
    "assert df_eval.iloc[2][\"Macro F1\"] > df_eval.iloc[0][\"Macro F1\"], \\\n",
    "    \"Keyword heuristic should beat random!\"\n",
    "print(\"Keyword heuristic beats random baseline: PASS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 55)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Total posts:               {len(df):,}\")\n",
    "print(f\"Annotators per post:       3\")\n",
    "print(f\"Label classes:             fearspeech, hatespeech, normal\")\n",
    "print(f\"  Normal posts:            {(df['primary_label']=='normal').sum():,}\")\n",
    "print(f\"  Hate speech posts:       {(df['primary_label']=='hatespeech').sum():,}\")\n",
    "print(f\"  Fear speech posts:       {(df['primary_label']=='fearspeech').sum():,}\")\n",
    "print(f\"  Fear+Hate overlap:       {(df['primary_label']=='fear+hate').sum():,}\")\n",
    "print(f\"Unique target communities: {len(target_counter)}\")\n",
    "print(f\"Top target:                {target_counter.most_common(1)[0][0]}\")\n",
    "print(f\"Emotion dimensions:        {len(emotion_cols)}\")\n",
    "print(f\"Mean word count:           {df['word_count'].mean():.1f}\")\n",
    "print(f\"Full annotator agreement:  {agreement_levels['full']/total*100:.1f}%\")\n",
    "print(f\"Train / Val / Test:        {len(split_ids['train']):,} / \"\n",
    "      f\"{len(split_ids['val']):,} / {len(split_ids['test']):,}\")\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Observations\n",
    "\n",
    "1. **Class imbalance:** Normal posts (44.8%) dominate, followed by hate\n",
    "   speech (34.6%) and fear speech (11.9%), with 7.8% overlap posts.\n",
    "   This imbalance reflects real-world distribution and makes fear speech\n",
    "   detection challenging.\n",
    "\n",
    "2. **Fear vs hate speech distinction:** Fear speech posts are longer on\n",
    "   average (46.4 words vs 40.1 for hate speech), consistent with the\n",
    "   paper's finding that fear speech uses narrative chains of argumentation\n",
    "   rather than direct insults.\n",
    "\n",
    "3. **Target communities:** Islam and Refugee communities are the most\n",
    "   frequently targeted, with distinct patterns: fear speech\n",
    "   disproportionately targets refugees (narrative of invasion), while\n",
    "   hate speech targets are more evenly distributed.\n",
    "\n",
    "4. **Emotion signatures:** Fear speech shows elevated *anger*,\n",
    "   *annoyance*, and *disappointment* scores compared to normal posts.\n",
    "   Notably, the emotion of *fear* itself is not strongly elevated,\n",
    "   confirming that fear speech is designed to instill fear in the\n",
    "   *audience* rather than expressing the speaker's fear.\n",
    "\n",
    "5. **Annotation difficulty:** Only 29.6% of posts achieve full\n",
    "   inter-annotator agreement, reflecting the inherently subjective\n",
    "   boundary between fear speech, hate speech, and normal posts.\n",
    "   The most common confusion is between fearspeech and hatespeech.\n",
    "\n",
    "6. **Research relevance (IS/AI):**\n",
    "   - **Content moderation:** Fear speech often evades toxicity filters\n",
    "     because it contains zero profanity \u2014 this dataset enables training\n",
    "     classifiers for implicit harmful speech.\n",
    "   - **AI safety alignment:** Training models to distinguish subtle\n",
    "     inflammatory rhetoric from overt hate speech is critical for\n",
    "     value-aligned AI systems.\n",
    "   - **Social simulation:** Understanding how fear speech outperforms\n",
    "     hate speech in network influence (more followers, more central\n",
    "     positions) informs agent-based social dynamics models."
   ]
  }
 ]
}