{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 1. Bitext Customer Support (Intent Recognition Benchmark)\n",
    "**Category:** AI Agent Core Capabilities\n",
    "\n",
    "**Source:** [Bitext / Customer Support LLM Chatbot Training Dataset](https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset)\n",
    "\n",
    "**Description:** Designed to train customer service agents in intent recognition\n",
    "and breakdown analysis, testing the AI's ability to accurately understand user needs.\n",
    "\n",
    "**Data Content:** Contains customer service corpora with 27 specific intents\n",
    "(e.g., order checks, refunds), 11 high-level categories, ~400 quality flag\n",
    "combinations, and templated dialog with dynamic placeholders.\n",
    "\n",
    "**License:** CDLA-Sharing-1.0\n",
    "\n",
    "---\n",
    "\n",
    "**This notebook covers:**\n",
    "1. Data loading from HuggingFace (26,872 instruction-response pairs)\n",
    "2. Schema exploration: categories, intents, flags, template placeholders\n",
    "3. Intent & category distribution analysis\n",
    "4. Instruction & response text length characteristics\n",
    "5. Quality flag decomposition and pattern analysis\n",
    "6. Template placeholder usage across intents\n",
    "7. Intent recognition agent evaluation framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install datasets pandas matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"axes.titlesize\"] = 13\n",
    "plt.rcParams[\"axes.labelsize\"] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview\n",
    "\n",
    "The Bitext Customer Support dataset is a **hybrid synthetic** corpus generated\n",
    "using NLP/NLG technology for fine-tuning LLMs in customer service applications.\n",
    "\n",
    "**Structure:**\n",
    "\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| `flags` | Quality/variation flags (394 unique combinations of 14 flag characters) |\n",
    "| `instruction` | Customer query with template placeholders (e.g., `{{Order Number}}`) |\n",
    "| `category` | High-level domain (11 categories: ORDER, ACCOUNT, REFUND, ...) |\n",
    "| `intent` | Specific intent label (27 intents: cancel_order, get_refund, ...) |\n",
    "| `response` | Agent response with dynamic placeholders |\n",
    "\n",
    "**Intent hierarchy (11 categories, 27 intents):**\n",
    "\n",
    "| Category | Intents |\n",
    "|----------|----------|\n",
    "| ACCOUNT | create_account, delete_account, edit_account, recover_password, registration_problems, switch_account |\n",
    "| ORDER | cancel_order, change_order, place_order, track_order |\n",
    "| REFUND | check_refund_policy, get_refund, track_refund |\n",
    "| INVOICE | check_invoice, get_invoice |\n",
    "| CONTACT | contact_customer_service, contact_human_agent |\n",
    "| PAYMENT | check_payment_methods, payment_issue |\n",
    "| FEEDBACK | complaint, review |\n",
    "| DELIVERY | delivery_options, delivery_period |\n",
    "| SHIPPING | change_shipping_address, set_up_shipping_address |\n",
    "| SUBSCRIPTION | newsletter_subscription |\n",
    "| CANCEL | check_cancellation_fee |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Bitext Customer Support dataset from HuggingFace...\")\n",
    "ds = load_dataset(\n",
    "    \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\",\n",
    "    split=\"train\"\n",
    ")\n",
    "df = ds.to_pandas()\n",
    "print(f\"Loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Data Schema & Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Data Types ===\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\n=== Unique Values ===\")\n",
    "for col in df.columns:\n",
    "    print(f\"  {col:15s}: {df[col].nunique()} unique\")\n",
    "print(f\"\\n=== Sample Rows ===\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show full instruction-response pairs for different intents\n",
    "sample_intents = [\"cancel_order\", \"get_refund\", \"recover_password\",\n",
    "                  \"delivery_options\", \"complaint\"]\n",
    "for intent in sample_intents:\n",
    "    row = df[df[\"intent\"] == intent].iloc[0]\n",
    "    print(f\"--- Intent: {intent} | Category: {row['category']} \"\n",
    "          f\"| Flags: {row['flags']} ---\")\n",
    "    print(f\"  Instruction: {row['instruction']}\")\n",
    "    print(f\"  Response:    {row['response'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "### 5.1 Category & Intent Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Category distribution\n",
    "cat_counts = df[\"category\"].value_counts()\n",
    "axes[0].barh(cat_counts.index, cat_counts.values, color=\"steelblue\",\n",
    "             edgecolor=\"white\")\n",
    "axes[0].set_title(\"Records per Category\")\n",
    "axes[0].set_xlabel(\"Count\")\n",
    "axes[0].invert_yaxis()\n",
    "for i, (idx, val) in enumerate(cat_counts.items()):\n",
    "    axes[0].text(val + 50, i, f\"{val:,}\", va=\"center\", fontsize=9)\n",
    "\n",
    "# Intent distribution\n",
    "intent_counts = df[\"intent\"].value_counts()\n",
    "axes[1].barh(intent_counts.index, intent_counts.values, color=\"coral\",\n",
    "             edgecolor=\"white\")\n",
    "axes[1].set_title(\"Records per Intent (27 intents)\")\n",
    "axes[1].set_xlabel(\"Count\")\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Categories: {df['category'].nunique()}, \"\n",
    "      f\"Intents: {df['intent'].nunique()}\")\n",
    "print(f\"Intents per category: \"\n",
    "      f\"min={df.groupby('category')['intent'].nunique().min()}, \"\n",
    "      f\"max={df.groupby('category')['intent'].nunique().max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intent-category heatmap\n",
    "cross = pd.crosstab(df[\"intent\"], df[\"category\"])\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(cross, annot=True, fmt=\"d\", cmap=\"YlOrRd\",\n",
    "            linewidths=0.5, cbar_kws={\"label\": \"Count\"})\n",
    "plt.title(\"Intent × Category Cross-Tabulation\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Intent\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### 5.2 Instruction & Response Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"instr_len\"] = df[\"instruction\"].apply(len)\n",
    "df[\"instr_words\"] = df[\"instruction\"].apply(lambda x: len(x.split()))\n",
    "df[\"resp_len\"] = df[\"response\"].apply(len)\n",
    "df[\"resp_words\"] = df[\"response\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].hist(df[\"instr_len\"], bins=50, color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[0, 0].set_title(f\"Instruction Length (chars)\\n\"\n",
    "                      f\"Mean={df['instr_len'].mean():.0f}, \"\n",
    "                      f\"Median={df['instr_len'].median():.0f}\")\n",
    "axes[0, 0].set_xlabel(\"Character Count\")\n",
    "axes[0, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[0, 1].hist(df[\"instr_words\"], bins=30, color=\"coral\", edgecolor=\"white\")\n",
    "axes[0, 1].set_title(f\"Instruction Length (words)\\n\"\n",
    "                      f\"Mean={df['instr_words'].mean():.1f}, \"\n",
    "                      f\"Median={df['instr_words'].median():.0f}\")\n",
    "axes[0, 1].set_xlabel(\"Word Count\")\n",
    "axes[0, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1, 0].hist(df[\"resp_len\"], bins=50, color=\"mediumseagreen\",\n",
    "                edgecolor=\"white\")\n",
    "axes[1, 0].set_title(f\"Response Length (chars)\\n\"\n",
    "                      f\"Mean={df['resp_len'].mean():.0f}, \"\n",
    "                      f\"Median={df['resp_len'].median():.0f}\")\n",
    "axes[1, 0].set_xlabel(\"Character Count\")\n",
    "axes[1, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1, 1].hist(df[\"resp_words\"], bins=50, color=\"orchid\", edgecolor=\"white\")\n",
    "axes[1, 1].set_title(f\"Response Length (words)\\n\"\n",
    "                      f\"Mean={df['resp_words'].mean():.1f}, \"\n",
    "                      f\"Median={df['resp_words'].median():.0f}\")\n",
    "axes[1, 1].set_xlabel(\"Word Count\")\n",
    "axes[1, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response length by category\n",
    "cat_order = (df.groupby(\"category\")[\"resp_words\"].median()\n",
    "             .sort_values(ascending=False).index)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df, x=\"category\", y=\"resp_words\", order=cat_order,\n",
    "            hue=\"category\", palette=\"Set2\", legend=False)\n",
    "plt.title(\"Response Length by Category (words)\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Word Count\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### 5.3 Quality Flag Analysis\n",
    "\n",
    "Each record has a `flags` string composed of single-letter codes. These\n",
    "represent data augmentation and quality variation markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose flag strings into individual characters\n",
    "flag_chars = Counter()\n",
    "for f in df[\"flags\"]:\n",
    "    for c in str(f):\n",
    "        flag_chars[c] += 1\n",
    "\n",
    "flag_df = (pd.DataFrame(flag_chars.items(), columns=[\"Flag\", \"Count\"])\n",
    "           .sort_values(\"Count\", ascending=False))\n",
    "\n",
    "df[\"n_flags\"] = df[\"flags\"].apply(len)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Individual flag frequency\n",
    "axes[0].bar(flag_df[\"Flag\"], flag_df[\"Count\"], color=\"steelblue\",\n",
    "            edgecolor=\"white\")\n",
    "axes[0].set_title(f\"Individual Flag Character Frequency ({len(flag_df)} unique)\")\n",
    "axes[0].set_xlabel(\"Flag Character\")\n",
    "axes[0].set_ylabel(\"Occurrences\")\n",
    "\n",
    "# Number of flags per record\n",
    "axes[1].hist(df[\"n_flags\"], bins=range(1, df[\"n_flags\"].max() + 2),\n",
    "             color=\"coral\", edgecolor=\"white\", align=\"left\")\n",
    "axes[1].set_title(\"Number of Flags per Record\")\n",
    "axes[1].set_xlabel(\"Flag Count\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Unique flag combinations: {df['flags'].nunique()}\")\n",
    "print(f\"Flag count: mean={df['n_flags'].mean():.1f}, \"\n",
    "      f\"min={df['n_flags'].min()}, max={df['n_flags'].max()}\")\n",
    "print(f\"\\nNote: 'B' appears in ALL records (baseline flag). \"\n",
    "      f\"'L' appears in {flag_chars['L']/len(df)*100:.0f}% of records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top flag combinations\n",
    "top_flags = df[\"flags\"].value_counts().head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "top_flags.plot(kind=\"barh\", color=\"mediumseagreen\", edgecolor=\"white\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Top 15 Flag Combinations\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Flag Combination\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 5.4 Template Placeholder Analysis\n",
    "\n",
    "Both instructions and responses use `{{placeholder}}` templates for\n",
    "dynamic content (order numbers, URLs, phone numbers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACEHOLDER_RE = re.compile(r\"\\{\\{[^}]+\\}\\}\")\n",
    "\n",
    "# Extract placeholders from instructions\n",
    "instr_ph = Counter()\n",
    "for text in df[\"instruction\"]:\n",
    "    for m in PLACEHOLDER_RE.findall(str(text)):\n",
    "        instr_ph[m] += 1\n",
    "\n",
    "# Extract placeholders from responses\n",
    "resp_ph = Counter()\n",
    "for text in df[\"response\"]:\n",
    "    for m in PLACEHOLDER_RE.findall(str(text)):\n",
    "        resp_ph[m] += 1\n",
    "\n",
    "df[\"n_instr_ph\"] = df[\"instruction\"].apply(\n",
    "    lambda x: len(PLACEHOLDER_RE.findall(str(x)))\n",
    ")\n",
    "df[\"n_resp_ph\"] = df[\"response\"].apply(\n",
    "    lambda x: len(PLACEHOLDER_RE.findall(str(x)))\n",
    ")\n",
    "\n",
    "print(f\"Unique placeholders in instructions: {len(instr_ph)}\")\n",
    "print(f\"Unique placeholders in responses: {len(resp_ph)}\")\n",
    "print(f\"\\n=== Top Instruction Placeholders ===\")\n",
    "for ph, cnt in instr_ph.most_common(10):\n",
    "    print(f\"  {ph:30s}: {cnt}\")\n",
    "print(f\"\\n=== Top Response Placeholders ===\")\n",
    "for ph, cnt in resp_ph.most_common(15):\n",
    "    print(f\"  {ph:45s}: {cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder usage distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df[\"n_instr_ph\"], bins=range(0, df[\"n_instr_ph\"].max() + 2),\n",
    "             color=\"steelblue\", edgecolor=\"white\", align=\"left\")\n",
    "axes[0].set_title(\"Placeholders per Instruction\")\n",
    "axes[0].set_xlabel(\"Number of Placeholders\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1].hist(df[\"n_resp_ph\"], bins=range(0, min(df[\"n_resp_ph\"].max() + 2, 20)),\n",
    "             color=\"orchid\", edgecolor=\"white\", align=\"left\")\n",
    "axes[1].set_title(\"Placeholders per Response\")\n",
    "axes[1].set_xlabel(\"Number of Placeholders\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Instructions with placeholders: \"\n",
    "      f\"{(df['n_instr_ph'] > 0).sum()} / {len(df)} \"\n",
    "      f\"({(df['n_instr_ph'] > 0).mean()*100:.1f}%)\")\n",
    "print(f\"Responses with placeholders: \"\n",
    "      f\"{(df['n_resp_ph'] > 0).sum()} / {len(df)} \"\n",
    "      f\"({(df['n_resp_ph'] > 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### 5.5 Instruction Diversity per Intent\n",
    "\n",
    "How many distinct instruction phrasings exist for each intent?\n",
    "This measures the paraphrase diversity of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique instructions per intent\n",
    "diversity = (df.groupby(\"intent\")\n",
    "             .agg(total=(\"instruction\", \"count\"),\n",
    "                  unique=(\"instruction\", \"nunique\"))\n",
    "             .assign(diversity_pct=lambda x: x[\"unique\"] / x[\"total\"] * 100)\n",
    "             .sort_values(\"diversity_pct\", ascending=False))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "axes[0].barh(diversity.index, diversity[\"unique\"], color=\"steelblue\",\n",
    "             edgecolor=\"white\")\n",
    "axes[0].set_title(\"Unique Instructions per Intent\")\n",
    "axes[0].set_xlabel(\"Unique Instruction Count\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "axes[1].barh(diversity.index, diversity[\"diversity_pct\"], color=\"coral\",\n",
    "             edgecolor=\"white\")\n",
    "axes[1].set_title(\"Instruction Diversity (% unique)\")\n",
    "axes[1].set_xlabel(\"Diversity %\")\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean diversity: {diversity['diversity_pct'].mean():.1f}%\")\n",
    "print(f\"Min diversity: {diversity['diversity_pct'].min():.1f}% \"\n",
    "      f\"({diversity['diversity_pct'].idxmin()})\")\n",
    "print(f\"Max diversity: {diversity['diversity_pct'].max():.1f}% \"\n",
    "      f\"({diversity['diversity_pct'].idxmax()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### 5.6 Instruction Length by Intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_order = (df.groupby(\"intent\")[\"instr_words\"].median()\n",
    "                .sort_values(ascending=False).index)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=df, y=\"intent\", x=\"instr_words\", order=intent_order,\n",
    "            hue=\"intent\", palette=\"tab20\", legend=False)\n",
    "plt.title(\"Instruction Length by Intent (words)\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Intent\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### 5.7 Response Complexity by Intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response characteristics per intent\n",
    "resp_stats = (df.groupby(\"intent\")\n",
    "              .agg(mean_words=(\"resp_words\", \"mean\"),\n",
    "                   mean_ph=(\"n_resp_ph\", \"mean\"),\n",
    "                   mean_chars=(\"resp_len\", \"mean\"))\n",
    "              .sort_values(\"mean_words\", ascending=False)\n",
    "              .round(1))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "axes[0].barh(resp_stats.index, resp_stats[\"mean_words\"],\n",
    "             color=\"mediumseagreen\", edgecolor=\"white\")\n",
    "axes[0].set_title(\"Mean Response Length per Intent (words)\")\n",
    "axes[0].set_xlabel(\"Mean Word Count\")\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "axes[1].barh(resp_stats.index, resp_stats[\"mean_ph\"],\n",
    "             color=\"orchid\", edgecolor=\"white\")\n",
    "axes[1].set_title(\"Mean Placeholders per Response by Intent\")\n",
    "axes[1].set_xlabel(\"Mean Placeholder Count\")\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 6. Intent Recognition Agent Evaluation Framework\n",
    "\n",
    "We define a scoring framework for evaluating how well an agent can\n",
    "correctly classify customer intents from instruction text.\n",
    "\n",
    "### Evaluation Criteria\n",
    "\n",
    "| Criterion | Metric | Weight | Description |\n",
    "|-----------|--------|--------|-------------|\n",
    "| **Intent Accuracy** | Exact match on 27 intents | 0.30 | Does the agent predict the correct intent? |\n",
    "| **Category Accuracy** | Exact match on 11 categories | 0.20 | Does the agent predict the correct category? |\n",
    "| **Robustness** | Accuracy on flagged variants | 0.20 | Consistent across typos/paraphrases? |\n",
    "| **Slot Extraction** | Placeholder recall | 0.15 | Does the agent extract template entities? |\n",
    "| **Response Quality** | BLEU/ROUGE against gold | 0.15 | Is the generated response appropriate? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline metrics from the dataset\n",
    "def compute_intent_baseline(df):\n",
    "    \"\"\"Compute baseline metrics for intent recognition.\"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # 1. Class balance (entropy-based)\n",
    "    intent_probs = df[\"intent\"].value_counts(normalize=True)\n",
    "    metrics[\"Intent Entropy\"] = -np.sum(\n",
    "        intent_probs * np.log2(intent_probs)\n",
    "    )\n",
    "    metrics[\"Max Possible Entropy\"] = np.log2(df[\"intent\"].nunique())\n",
    "    metrics[\"Balance Ratio\"] = (\n",
    "        metrics[\"Intent Entropy\"] / metrics[\"Max Possible Entropy\"]\n",
    "    )\n",
    "\n",
    "    # 2. Instruction diversity\n",
    "    metrics[\"Unique Instructions\"] = df[\"instruction\"].nunique()\n",
    "    metrics[\"Instruction Diversity %\"] = (\n",
    "        df[\"instruction\"].nunique() / len(df) * 100\n",
    "    )\n",
    "\n",
    "    # 3. Placeholder coverage\n",
    "    metrics[\"Instructions with Slots\"] = (\n",
    "        (df[\"n_instr_ph\"] > 0).mean() * 100\n",
    "    )\n",
    "    metrics[\"Responses with Slots\"] = (\n",
    "        (df[\"n_resp_ph\"] > 0).mean() * 100\n",
    "    )\n",
    "\n",
    "    # 4. Mean flag complexity\n",
    "    metrics[\"Mean Flag Count\"] = df[\"n_flags\"].mean()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "baseline = compute_intent_baseline(df)\n",
    "print(\"=== Intent Recognition Baseline Metrics ===\")\n",
    "for k, v in baseline.items():\n",
    "    print(f\"  {k:30s}: {v:.4f}\" if isinstance(v, float) else\n",
    "          f\"  {k:30s}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate agent strategies for intent classification\n",
    "np.random.seed(42)\n",
    "all_intents = df[\"intent\"].unique()\n",
    "all_categories = df[\"category\"].unique()\n",
    "intent_to_cat = df.groupby(\"intent\")[\"category\"].first().to_dict()\n",
    "\n",
    "def evaluate_agent(predictions, true_intents, true_categories):\n",
    "    \"\"\"Evaluate intent classification agent.\"\"\"\n",
    "    pred_intents = [p[\"intent\"] for p in predictions]\n",
    "    pred_categories = [intent_to_cat.get(pi, \"UNKNOWN\") for pi in pred_intents]\n",
    "\n",
    "    intent_acc = np.mean(\n",
    "        [p == t for p, t in zip(pred_intents, true_intents)]\n",
    "    )\n",
    "    cat_acc = np.mean(\n",
    "        [p == t for p, t in zip(pred_categories, true_categories)]\n",
    "    )\n",
    "    return {\"Intent Accuracy\": intent_acc, \"Category Accuracy\": cat_acc}\n",
    "\n",
    "\n",
    "# Sample test set\n",
    "test_df = df.sample(1000, random_state=42)\n",
    "\n",
    "# Strategy 1: Random baseline\n",
    "random_preds = [{\"intent\": np.random.choice(all_intents)} for _ in range(len(test_df))]\n",
    "\n",
    "# Strategy 2: Most-frequent class\n",
    "most_common_intent = df[\"intent\"].mode()[0]\n",
    "majority_preds = [{\"intent\": most_common_intent} for _ in range(len(test_df))]\n",
    "\n",
    "# Strategy 3: Keyword matching (simple heuristic)\n",
    "keyword_map = {\n",
    "    \"cancel\": \"cancel_order\", \"refund\": \"get_refund\",\n",
    "    \"track\": \"track_order\", \"invoice\": \"check_invoice\",\n",
    "    \"password\": \"recover_password\", \"delivery\": \"delivery_period\",\n",
    "    \"shipping\": \"set_up_shipping_address\", \"payment\": \"payment_issue\",\n",
    "    \"complaint\": \"complaint\", \"review\": \"review\",\n",
    "    \"account\": \"edit_account\", \"subscribe\": \"newsletter_subscription\",\n",
    "    \"human\": \"contact_human_agent\", \"contact\": \"contact_customer_service\",\n",
    "    \"place\": \"place_order\", \"change\": \"change_order\",\n",
    "}\n",
    "\n",
    "def keyword_predict(instruction):\n",
    "    instr_lower = instruction.lower()\n",
    "    for kw, intent in keyword_map.items():\n",
    "        if kw in instr_lower:\n",
    "            return {\"intent\": intent}\n",
    "    return {\"intent\": most_common_intent}\n",
    "\n",
    "keyword_preds = [keyword_predict(row[\"instruction\"])\n",
    "                 for _, row in test_df.iterrows()]\n",
    "\n",
    "# Evaluate all strategies\n",
    "strategies = {\n",
    "    \"Random Baseline\": random_preds,\n",
    "    \"Majority Class\": majority_preds,\n",
    "    \"Keyword Matching\": keyword_preds,\n",
    "}\n",
    "\n",
    "eval_results = []\n",
    "for name, preds in strategies.items():\n",
    "    scores = evaluate_agent(\n",
    "        preds,\n",
    "        test_df[\"intent\"].tolist(),\n",
    "        test_df[\"category\"].tolist()\n",
    "    )\n",
    "    eval_results.append({\"Strategy\": name, **scores})\n",
    "\n",
    "df_eval = pd.DataFrame(eval_results)\n",
    "print(\"=== Intent Recognition Agent Evaluation ===\")\n",
    "print(df_eval.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart comparing strategies\n",
    "labels = [\"Intent Accuracy\", \"Category Accuracy\"]\n",
    "n_metrics = len(labels)\n",
    "angles = np.linspace(0, 2 * np.pi, n_metrics, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7), subplot_kw=dict(polar=True))\n",
    "colors_strat = [\"gray\", \"coral\", \"steelblue\"]\n",
    "\n",
    "for i, (_, row) in enumerate(df_eval.iterrows()):\n",
    "    values = [row[l] for l in labels]\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, \"o-\", linewidth=2, color=colors_strat[i],\n",
    "            markersize=8, label=row[\"Strategy\"])\n",
    "    ax.fill(angles, values, alpha=0.1, color=colors_strat[i])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(labels, fontsize=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels([\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], fontsize=8)\n",
    "ax.set_title(\"Intent Recognition Strategy Comparison\", fontsize=14, pad=20)\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## 7. Summary & Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BITEXT CUSTOMER SUPPORT BENCHMARK - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n[Data Scope]\")\n",
    "print(f\"  Total records: {len(df):,}\")\n",
    "print(f\"  Categories: {df['category'].nunique()}\")\n",
    "print(f\"  Intents: {df['intent'].nunique()}\")\n",
    "print(f\"  Unique instructions: {df['instruction'].nunique():,}\")\n",
    "print(f\"  Unique responses: {df['response'].nunique():,}\")\n",
    "print(f\"  Flag combinations: {df['flags'].nunique()}\")\n",
    "\n",
    "print(f\"\\n[Text Statistics]\")\n",
    "print(f\"  Instruction: mean={df['instr_words'].mean():.1f} words, \"\n",
    "      f\"median={df['instr_words'].median():.0f}\")\n",
    "print(f\"  Response: mean={df['resp_words'].mean():.1f} words, \"\n",
    "      f\"median={df['resp_words'].median():.0f}\")\n",
    "\n",
    "print(f\"\\n[Template Placeholders]\")\n",
    "print(f\"  Unique in instructions: {len(instr_ph)}\")\n",
    "print(f\"  Unique in responses: {len(resp_ph)}\")\n",
    "print(f\"  Instructions with slots: {(df['n_instr_ph']>0).mean()*100:.1f}%\")\n",
    "print(f\"  Responses with slots: {(df['n_resp_ph']>0).mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n[Agent Evaluation]\")\n",
    "for _, row in df_eval.iterrows():\n",
    "    print(f\"  {row['Strategy']:20s}: Intent={row['Intent Accuracy']:.3f}, \"\n",
    "          f\"Category={row['Category Accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## 8. Key Observations\n",
    "\n",
    "1. **Balanced intents:** The dataset is remarkably well-balanced across 27 intents\n",
    "   (~950-1,000 samples each), providing fair training signal for all categories.\n",
    "   The balance ratio (entropy / max entropy) is near 1.0.\n",
    "\n",
    "2. **Rich paraphrase diversity:** Each intent has hundreds of unique instruction\n",
    "   phrasings (including typos, abbreviations, and colloquial variants marked by\n",
    "   quality flags), simulating real user behavior.\n",
    "\n",
    "3. **Template-driven responses:** Responses use dynamic `{{placeholder}}` slots\n",
    "   (order numbers, URLs, phone numbers), enabling evaluation of both intent\n",
    "   classification and slot-filling capabilities.\n",
    "\n",
    "4. **Hierarchical structure:** The 2-level category-intent hierarchy allows\n",
    "   evaluation at different granularities — coarse (11 categories) vs fine (27 intents).\n",
    "\n",
    "5. **Flag-based robustness testing:** The 394 flag combinations encode augmentation\n",
    "   patterns (typos, length variants, paraphrases), enabling systematic robustness evaluation.\n",
    "\n",
    "6. **Research relevance (IS/AI):**\n",
    "   - **Intent recognition:** Train and benchmark customer service intent classifiers\n",
    "   - **Slot filling:** Evaluate entity extraction from templated conversations\n",
    "   - **Chatbot fine-tuning:** Directly fine-tune LLMs for customer support\n",
    "   - **Robustness testing:** Assess model stability across paraphrase variants\n",
    "   - **Dialog breakdown detection:** Use flag patterns to identify potential failure points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}