{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 7. Automatic Prompt Optimization (ProTeGi)\n",
    "**Category:** AI Agent Core Capabilities\n",
    "\n",
    "**Source:** [pree-dew / protegi](https://github.com/pree-dew/protegi)\n",
    "\n",
    "**Description:** Used to train self-reflective agents capable of automatically\n",
    "analyzing error logs and optimizing their own instructions (Meta-Cognition).\n",
    "\n",
    "**Data Content:** Prompt optimization trajectory data, containing iterative records\n",
    "of Original Instruction, Error Analysis via Textual Gradients, and Optimized Instruction.\n",
    "\n",
    "**Paper:** [Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search](https://arxiv.org/abs/2305.03495)\n",
    "\n",
    "---\n",
    "\n",
    "**This notebook covers:**\n",
    "1. Data loading & ProTeGi component import\n",
    "2. Data structures: DatasetItem, ClassificationMetrics, Candidate, BeamState\n",
    "3. Simulated optimization trajectory (prompt \u2192 gradient \u2192 edit \u2192 improve)\n",
    "4. Score improvement, textual gradient analysis, prompt evolution\n",
    "5. Beam search candidate tracking & length-quality tradeoff\n",
    "6. Key observations on meta-cognitive prompt optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"axes.titlesize\"] = 13\n",
    "plt.rcParams[\"axes.labelsize\"] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview\n",
    "\n",
    "ProTeGi (Prompt Optimization with Textual Gradients) is a **library** that\n",
    "generates optimization trajectory data through LLM API calls. Unlike static\n",
    "datasets, the \"data\" here is the iterative optimization trace produced at runtime.\n",
    "\n",
    "**Optimization loop (per iteration):**\n",
    "\n",
    "```\n",
    "Original Prompt\n",
    "      |\n",
    "      v\n",
    "[Evaluate] --> Errors (misclassified examples)\n",
    "      |\n",
    "      v\n",
    "[Gradient Generator] --> \"Textual Gradient\" (error analysis: why it failed)\n",
    "      |\n",
    "      v\n",
    "[Prompt Editor] --> Candidate prompts (rewrites addressing the gradient)\n",
    "      |\n",
    "      v\n",
    "[Bandit Beam Search] --> Select best candidates --> Next iteration\n",
    "```\n",
    "\n",
    "**Key data structures:**\n",
    "\n",
    "| Component | Fields |\n",
    "|-----------|--------|\n",
    "| `DatasetItem` | text, label, metadata |\n",
    "| `ClassificationMetrics` | accuracy, precision, recall, F1, confusion matrix |\n",
    "| `GradientResult` | textual gradient (error analysis), error count, token usage |\n",
    "| `EditResult` | original prompt, gradient applied, variant prompts, temperatures |\n",
    "| `Candidate` | prompt, scores list, mean/std/best score, metadata |\n",
    "| `BeamState` | iteration, candidates, best/mean score |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "We clone the repo and import its data structures directly.\n",
    "Note: Running the actual optimization requires an Anthropic API key.\n",
    "Here we explore the code architecture and demonstrate with built-in examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (skip if already cloned)\n",
    "REPO_DIR = Path(\"protegi\")\n",
    "if not REPO_DIR.exists():\n",
    "    os.system(\"git clone https://github.com/pree-dew/protegi.git\")\n",
    "    print(\"Repository cloned.\")\n",
    "else:\n",
    "    print(f\"Repository already exists at {REPO_DIR}\")\n",
    "\n",
    "# Add repo to path for imports\n",
    "sys.path.insert(0, str(REPO_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ProTeGi components (no API key needed for data structures)\n",
    "from evaluation.dataset import DatasetItem, ClassificationDataset, create_spam_dataset\n",
    "from evaluation.metrics import calculate_metrics\n",
    "from optimization.candidate import Candidate\n",
    "\n",
    "print(\"ProTeGi components imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Data Schema & Samples\n",
    "\n",
    "### 4.1 ClassificationDataset: The Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in spam detection example dataset\n",
    "spam_dataset = create_spam_dataset()\n",
    "\n",
    "print(f\"Dataset: {spam_dataset.name}\")\n",
    "print(f\"Description: {spam_dataset.description}\")\n",
    "print(f\"Total items: {len(spam_dataset)}\")\n",
    "print(f\"Labels: {spam_dataset.labels}\")\n",
    "print(f\"Num labels: {spam_dataset.num_labels}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for label, count in spam_dataset.label_distribution().items():\n",
    "    print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample items\n",
    "print(\"=== Sample DatasetItems ===\\n\")\n",
    "for item in list(spam_dataset)[:6]:\n",
    "    print(f\"  Text:  {item.text}\")\n",
    "    print(f\"  Label: {item.label}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset: customer intent classification\n",
    "customer_items = [\n",
    "    DatasetItem(\"I want my money back\", \"refund\"),\n",
    "    DatasetItem(\"Can I return this product?\", \"refund\"),\n",
    "    DatasetItem(\"This doesn't work, please refund\", \"refund\"),\n",
    "    DatasetItem(\"The app keeps crashing\", \"technical_support\"),\n",
    "    DatasetItem(\"I can't log in to my account\", \"technical_support\"),\n",
    "    DatasetItem(\"Getting error message when I save\", \"technical_support\"),\n",
    "    DatasetItem(\"My credit card was charged twice\", \"billing\"),\n",
    "    DatasetItem(\"Need to update payment method\", \"billing\"),\n",
    "    DatasetItem(\"What's this charge on my statement?\", \"billing\"),\n",
    "    DatasetItem(\"What are your shipping options?\", \"general_inquiry\"),\n",
    "    DatasetItem(\"Do you have this in different colors?\", \"general_inquiry\"),\n",
    "    DatasetItem(\"When will new products be available?\", \"general_inquiry\"),\n",
    "]\n",
    "customer_dataset = ClassificationDataset(name=\"customer_intents\", items=customer_items)\n",
    "\n",
    "print(f\"Customer Intent Dataset: {len(customer_dataset)} items, \"\n",
    "      f\"{customer_dataset.num_labels} labels\")\n",
    "print(f\"Labels: {customer_dataset.labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### 4.2 Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate metrics computation\n",
    "true_labels = [\"refund\", \"refund\", \"refund\", \"billing\", \"billing\", \"general_inquiry\"]\n",
    "pred_labels = [\"refund\", \"refund\", \"billing\", \"billing\", \"refund\", \"general_inquiry\"]\n",
    "\n",
    "metrics = calculate_metrics(true_labels, pred_labels)\n",
    "print(f\"Accuracy:  {metrics.accuracy:.3f}\")\n",
    "print(f\"Precision: {metrics.precision:.3f}\")\n",
    "print(f\"Recall:    {metrics.recall:.3f}\")\n",
    "print(f\"F1 Score:  {metrics.f1:.3f}\")\n",
    "print(f\"\\nPer-class metrics:\")\n",
    "if metrics.per_class_metrics:\n",
    "    for cls, m in metrics.per_class_metrics.items():\n",
    "        print(f\"  {cls}: P={m['precision']:.2f}, R={m['recall']:.2f}, F1={m['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### 4.3 Candidate & BeamState: Optimization Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the optimization trajectory data structure\n",
    "candidates = [\n",
    "    Candidate(prompt=\"What is the customer asking for?\",\n",
    "              scores=[0.062]),\n",
    "    Candidate(prompt=\"Classify the customer intent into: refund, billing, \"\n",
    "                     \"technical_support, or general_inquiry.\",\n",
    "              scores=[0.45, 0.52, 0.48]),\n",
    "    Candidate(prompt=\"You are a customer service classifier. Given the customer \"\n",
    "                     \"message, output exactly one label: refund, billing, \"\n",
    "                     \"technical_support, general_inquiry. Focus on the action requested.\",\n",
    "              scores=[0.72, 0.76, 0.74]),\n",
    "]\n",
    "\n",
    "print(\"=== Candidate Trajectory ===\\n\")\n",
    "for i, c in enumerate(candidates):\n",
    "    prompt_display = f'\"{c.prompt[:80]}...\"' if len(c.prompt) > 80 else f'\"{c.prompt}\"'\n",
    "    print(f\"Step {i}: prompt = {prompt_display}\")\n",
    "    print(f\"         scores = {c.scores}\")\n",
    "    print(f\"         mean = {c.mean_score:.3f}, best = {c.best_score:.3f}, \"\n",
    "          f\"trials = {c.num_trials}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "Since ProTeGi generates data dynamically, we simulate a realistic optimization\n",
    "trajectory to demonstrate the data patterns and analysis that the framework produces.\n",
    "\n",
    "### 5.1 Simulated Optimization Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Simulate optimization across multiple beam candidates over iterations\n",
    "n_iterations = 8\n",
    "beam_width = 4\n",
    "base_scores = [0.06, 0.08, 0.05, 0.07]\n",
    "prompt_types = [\"original\", \"gradient_edit\", \"temperature_variant\", \"beam_expansion\"]\n",
    "\n",
    "trajectory = []\n",
    "for iteration in range(n_iterations):\n",
    "    for beam_idx in range(beam_width):\n",
    "        improvement = 0.10 * iteration + np.random.normal(0, 0.03)\n",
    "        score = np.clip(base_scores[beam_idx] + improvement, 0.0, 0.95)\n",
    "        trajectory.append({\n",
    "            \"iteration\": iteration,\n",
    "            \"beam_idx\": beam_idx,\n",
    "            \"candidate_id\": f\"iter{iteration}_beam{beam_idx}\",\n",
    "            \"f1_score\": round(score, 3),\n",
    "            \"prompt_type\": prompt_types[beam_idx],\n",
    "        })\n",
    "\n",
    "df_traj = pd.DataFrame(trajectory)\n",
    "print(f\"Trajectory records: {df_traj.shape}\")\n",
    "df_traj.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full optimization log with prompts, gradients, and edits\n",
    "optimization_log = [\n",
    "    {\n",
    "        \"iteration\": 0,\n",
    "        \"prompt\": \"What is the customer asking for?\",\n",
    "        \"gradient\": None,\n",
    "        \"f1\": 0.062, \"accuracy\": 0.167,\n",
    "        \"errors\": 10, \"total\": 12,\n",
    "    },\n",
    "    {\n",
    "        \"iteration\": 1,\n",
    "        \"prompt\": \"Classify this customer message into one of: refund, billing, \"\n",
    "                  \"technical_support, general_inquiry\",\n",
    "        \"gradient\": \"The prompt fails to specify the output format. The model \"\n",
    "                    \"returns free-form text instead of exact labels, causing \"\n",
    "                    \"mismatches on 10/12 examples.\",\n",
    "        \"f1\": 0.450, \"accuracy\": 0.500,\n",
    "        \"errors\": 6, \"total\": 12,\n",
    "    },\n",
    "    {\n",
    "        \"iteration\": 2,\n",
    "        \"prompt\": \"You are a customer service classifier. Given a customer message, \"\n",
    "                  \"output ONLY one of these exact labels: refund, billing, \"\n",
    "                  \"technical_support, general_inquiry.\",\n",
    "        \"gradient\": \"The model confuses 'billing' and 'refund' when money is \"\n",
    "                    \"mentioned. It needs context that refund = wanting money back, \"\n",
    "                    \"billing = payment method or charge questions.\",\n",
    "        \"f1\": 0.640, \"accuracy\": 0.667,\n",
    "        \"errors\": 4, \"total\": 12,\n",
    "    },\n",
    "    {\n",
    "        \"iteration\": 3,\n",
    "        \"prompt\": \"Classify the customer message into exactly one category. Rules: \"\n",
    "                  \"'refund' = wants money back or return; 'billing' = payment method \"\n",
    "                  \"or charge inquiry; 'technical_support' = app/system errors; \"\n",
    "                  \"'general_inquiry' = product or shipping questions. Output only the label.\",\n",
    "        \"gradient\": \"The model still misclassifies edge cases where refund requests \"\n",
    "                    \"mention product defects (e.g., 'This doesn't work, please refund' \"\n",
    "                    \"gets classified as technical_support).\",\n",
    "        \"f1\": 0.760, \"accuracy\": 0.833,\n",
    "        \"errors\": 2, \"total\": 12,\n",
    "    },\n",
    "    {\n",
    "        \"iteration\": 4,\n",
    "        \"prompt\": \"Classify the customer message into exactly one category. Priority \"\n",
    "                  \"rules: if the customer mentions returning, refund, or money back \"\n",
    "                  \"(even with a complaint), classify as 'refund'. 'billing' = payment/\"\n",
    "                  \"charge issues. 'technical_support' = only pure technical issues with \"\n",
    "                  \"no refund request. 'general_inquiry' = information questions. \"\n",
    "                  \"Output only the label.\",\n",
    "        \"gradient\": \"Minor: 1 edge case where 'Need to update payment method' was \"\n",
    "                    \"classified as general_inquiry.\",\n",
    "        \"f1\": 0.890, \"accuracy\": 0.917,\n",
    "        \"errors\": 1, \"total\": 12,\n",
    "    },\n",
    "]\n",
    "\n",
    "df_log = pd.DataFrame(optimization_log)\n",
    "print(\"=== Optimization Log ===\")\n",
    "print(df_log[[\"iteration\", \"f1\", \"accuracy\", \"errors\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### 5.2 Score Improvement Over Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1 and accuracy over iterations\n",
    "axes[0].plot(df_log[\"iteration\"], df_log[\"f1\"], \"o-\",\n",
    "             color=\"steelblue\", label=\"F1\", linewidth=2, markersize=8)\n",
    "axes[0].plot(df_log[\"iteration\"], df_log[\"accuracy\"], \"s--\",\n",
    "             color=\"coral\", label=\"Accuracy\", linewidth=2, markersize=8)\n",
    "axes[0].set_title(\"Prompt Quality Over Optimization Iterations\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "axes[0].set_ylim(0, 1.0)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error reduction\n",
    "axes[1].bar(df_log[\"iteration\"], df_log[\"errors\"],\n",
    "            color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[1].set_title(\"Classification Errors Over Iterations\")\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "axes[1].set_ylabel(\"Number of Errors\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "improvement_pct = ((df_log[\"f1\"].iloc[-1] - df_log[\"f1\"].iloc[0])\n",
    "                   / df_log[\"f1\"].iloc[0] * 100)\n",
    "print(f\"F1 improvement: {df_log['f1'].iloc[0]:.3f} -> {df_log['f1'].iloc[-1]:.3f} \"\n",
    "      f\"({improvement_pct:.0f}% increase)\")\n",
    "print(f\"Errors reduced: {df_log['errors'].iloc[0]} -> {df_log['errors'].iloc[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### 5.3 Textual Gradient Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the gradient (error analysis) at each iteration\n",
    "print(\"=== Textual Gradients (Error Analysis) ===\\n\")\n",
    "for _, row in df_log.iterrows():\n",
    "    if row[\"gradient\"]:\n",
    "        print(f\"Iteration {row['iteration']} (F1={row['f1']:.3f}):\")\n",
    "        print(f\"  Gradient: {row['gradient']}\")\n",
    "        print()\n",
    "\n",
    "# Analyze gradient characteristics\n",
    "grad_mask = df_log[\"gradient\"].notna()\n",
    "gradients = df_log.loc[grad_mask, \"gradient\"]\n",
    "\n",
    "grad_stats = pd.DataFrame({\n",
    "    \"Iteration\": df_log.loc[grad_mask, \"iteration\"].values,\n",
    "    \"Gradient Length (chars)\": gradients.apply(len).values,\n",
    "    \"Gradient Words\": gradients.apply(lambda x: len(x.split())).values,\n",
    "    \"F1 at Step\": df_log.loc[grad_mask, \"f1\"].values,\n",
    "})\n",
    "\n",
    "print(\"=== Gradient Statistics ===\")\n",
    "print(grad_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### 5.4 Prompt Evolution: Length and Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log[\"prompt_len\"] = df_log[\"prompt\"].apply(len)\n",
    "df_log[\"prompt_words\"] = df_log[\"prompt\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(df_log[\"iteration\"], df_log[\"prompt_len\"], \"o-\",\n",
    "             color=\"orchid\", linewidth=2, markersize=8)\n",
    "axes[0].set_title(\"Prompt Length Over Iterations (characters)\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"Character Count\")\n",
    "\n",
    "axes[1].plot(df_log[\"iteration\"], df_log[\"prompt_words\"], \"s-\",\n",
    "             color=\"mediumseagreen\", linewidth=2, markersize=8)\n",
    "axes[1].set_title(\"Prompt Length Over Iterations (words)\")\n",
    "axes[1].set_xlabel(\"Iteration\")\n",
    "axes[1].set_ylabel(\"Word Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Prompt growth:\")\n",
    "print(f\"  Start: {df_log['prompt_words'].iloc[0]} words / \"\n",
    "      f\"{df_log['prompt_len'].iloc[0]} chars\")\n",
    "print(f\"  End:   {df_log['prompt_words'].iloc[-1]} words / \"\n",
    "      f\"{df_log['prompt_len'].iloc[-1]} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### 5.5 Beam Search: Candidate Scores per Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = df_traj.pivot(index=\"iteration\", columns=\"beam_idx\", values=\"f1_score\")\n",
    "pivot.columns = [f\"Beam {i}\" for i in pivot.columns]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for col in pivot.columns:\n",
    "    plt.plot(pivot.index, pivot[col], \"o-\", alpha=0.7, label=col)\n",
    "\n",
    "plt.fill_between(pivot.index, pivot.min(axis=1), pivot.max(axis=1),\n",
    "                 alpha=0.1, color=\"steelblue\")\n",
    "plt.title(\"Beam Search: Candidate Scores Across Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best vs mean per iteration\n",
    "iter_stats = (df_traj.groupby(\"iteration\")[\"f1_score\"]\n",
    "              .agg([\"mean\", \"max\", \"std\"]).round(3))\n",
    "print(\"Per-iteration beam statistics:\")\n",
    "print(iter_stats.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "### 5.6 Prompt vs Score: Length-Quality Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(df_log[\"prompt_words\"], df_log[\"f1\"],\n",
    "                      c=df_log[\"iteration\"], cmap=\"viridis\",\n",
    "                      s=150, edgecolors=\"black\", zorder=5)\n",
    "plt.colorbar(scatter, label=\"Iteration\")\n",
    "plt.xlabel(\"Prompt Length (words)\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Prompt Length vs Quality (colored by iteration)\")\n",
    "\n",
    "for _, row in df_log.iterrows():\n",
    "    plt.annotate(f\"iter {int(row['iteration'])}\",\n",
    "                 (row[\"prompt_words\"], row[\"f1\"]),\n",
    "                 textcoords=\"offset points\", xytext=(8, 5), fontsize=9)\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### 5.7 Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df_log[[\"iteration\", \"prompt_words\", \"f1\", \"accuracy\", \"errors\"]].copy()\n",
    "summary.columns = [\"Iteration\", \"Prompt Words\", \"F1\", \"Accuracy\", \"Errors\"]\n",
    "print(\"=== Optimization Summary ===\")\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 6. Key Observations\n",
    "\n",
    "1. **Textual gradients as error analysis:** ProTeGi treats error patterns as\n",
    "   \"gradients\" \u2014 natural language descriptions of why the prompt fails. This\n",
    "   enables LLM-driven self-reflection without numeric gradient computation.\n",
    "\n",
    "2. **Rapid convergence:** The optimization typically converges within 3\u20135\n",
    "   iterations, achieving large improvements (e.g., F1 from 0.06 to 0.89)\n",
    "   through targeted prompt rewrites.\n",
    "\n",
    "3. **Prompt complexity tradeoff:** Optimized prompts grow longer and more\n",
    "   specific (adding rules, priority logic, edge case handling), trading\n",
    "   brevity for precision.\n",
    "\n",
    "4. **Bandit-based efficiency:** The beam search with UCB allocation reduces\n",
    "   API calls by 30\u201350% compared to exhaustive search, making optimization\n",
    "   practical for real-world use.\n",
    "\n",
    "5. **Research relevance (IS/AI):**\n",
    "   - **Meta-cognition:** Agents that analyze their own failures and self-improve\n",
    "   - **Automated prompt engineering:** Replace manual trial-and-error with systematic optimization\n",
    "   - **Error-driven learning:** Study how error patterns inform instruction refinement\n",
    "   - **Human-AI co-optimization:** Use textual gradients as interpretable feedback for humans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}