{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 6. ThoughtSource (Chain-of-Thought Reasoning Benchmark)\n",
    "**Category:** AI Agent Core Capabilities\n",
    "\n",
    "**Source:** [OpenBioLink / ThoughtSource](https://github.com/OpenBioLink/ThoughtSource)\n",
    "\n",
    "**Description:** Designed to enhance agent reasoning capabilities using\n",
    "Chain-of-Thought (CoT) data, teaching models to think through steps before answering.\n",
    "\n",
    "**Data Content:** A collection of triplets consisting of Questions, Detailed\n",
    "Rationales (intermediate reasoning steps), and Final Answers across 14+ source datasets.\n",
    "\n",
    "**Paper:** [ThoughtSource: A central hub for large language model reasoning data](https://arxiv.org/abs/2301.11596)\n",
    "\n",
    "---\n",
    "\n",
    "**This notebook covers:**\n",
    "1. Data loading from HuggingFace (4 representative datasets)\n",
    "2. Schema comparison: built-in CoT vs generated CoT\n",
    "3. Rationale analysis: step count, length, depth\n",
    "4. Question length & answer distributions across domains\n",
    "5. GSM8K vs AQUA-RAT reasoning depth comparison\n",
    "6. Cross-dataset CoT availability summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install datasets pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"axes.titlesize\"] = 13\n",
    "plt.rcParams[\"axes.labelsize\"] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview\n",
    "\n",
    "ThoughtSource unifies **14+ datasets** across 3 reasoning domains into a common\n",
    "(Question, Chain-of-Thought, Answer) schema. Here we load 4 representative\n",
    "source datasets directly from HuggingFace:\n",
    "\n",
    "| Dataset | Domain | CoT Field | Records |\n",
    "|---------|--------|-----------|----------|\n",
    "| **GSM8K** | Math | answer (contains step-by-step + final) | ~8.8k |\n",
    "| **AQUA-RAT** | Math | `rationale` (explicit reasoning) | ~98k |\n",
    "| **CommonsenseQA** | General QA | (ThoughtSource adds generated CoT) | ~12k |\n",
    "| **OpenBookQA** | Science | (ThoughtSource adds generated CoT) | ~6k |\n",
    "\n",
    "The full ThoughtSource collection also includes: WorldTree, EntailmentBank,\n",
    "StrategyQA, QED, MedQA, MedMCQA, MMLU, PubMedQA, ASDIV, MAWPS, SVAMP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading GSM8K (math reasoning with step-by-step solutions)...\")\n",
    "ds_gsm8k = load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "print(\"Loading AQUA-RAT (math with explicit rationales)...\")\n",
    "ds_aqua = load_dataset(\"aqua_rat\", \"raw\")\n",
    "\n",
    "print(\"Loading CommonsenseQA (multiple-choice commonsense)...\")\n",
    "ds_csqa = load_dataset(\"commonsense_qa\")\n",
    "\n",
    "print(\"Loading OpenBookQA (science multiple-choice)...\")\n",
    "ds_obqa = load_dataset(\"allenai/openbookqa\", \"main\")\n",
    "\n",
    "print(\"\\nAll datasets loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sizes per split\n",
    "datasets_map = {\n",
    "    \"GSM8K\": ds_gsm8k,\n",
    "    \"AQUA-RAT\": ds_aqua,\n",
    "    \"CommonsenseQA\": ds_csqa,\n",
    "    \"OpenBookQA\": ds_obqa,\n",
    "}\n",
    "\n",
    "for name, ds in datasets_map.items():\n",
    "    splits_info = {k: len(v) for k, v in ds.items()}\n",
    "    print(f\"{name:15s} {splits_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Data Schema & Samples\n",
    "\n",
    "### 4.1 GSM8K — Step-by-step math reasoning\n",
    "The `answer` field contains the full rationale followed by `#### <final_answer>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== GSM8K ===\")\n",
    "print(f\"Columns: {ds_gsm8k['train'].column_names}\\n\")\n",
    "\n",
    "for i in range(2):\n",
    "    item = ds_gsm8k[\"train\"][i]\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(f\"Question: {item['question']}\")\n",
    "    print(f\"Answer (rationale + final):\\n{item['answer']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### 4.2 AQUA-RAT — Explicit `rationale` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== AQUA-RAT ===\")\n",
    "print(f\"Columns: {ds_aqua['train'].column_names}\\n\")\n",
    "\n",
    "item = ds_aqua[\"train\"][0]\n",
    "print(f\"Question: {item['question']}\")\n",
    "print(f\"Options: {item['options']}\")\n",
    "print(f\"Rationale: {item['rationale']}\")\n",
    "print(f\"Correct: {item['correct']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### 4.3 CommonsenseQA — Multiple choice, no built-in CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CommonsenseQA ===\")\n",
    "print(f\"Columns: {ds_csqa['train'].column_names}\\n\")\n",
    "\n",
    "item = ds_csqa[\"train\"][0]\n",
    "print(f\"Question: {item['question']}\")\n",
    "print(f\"Concept: {item['question_concept']}\")\n",
    "print(f\"Choices: {list(zip(item['choices']['label'], item['choices']['text']))}\")\n",
    "print(f\"Answer: {item['answerKey']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### 4.4 OpenBookQA — Science multiple choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== OpenBookQA ===\")\n",
    "print(f\"Columns: {ds_obqa['train'].column_names}\\n\")\n",
    "\n",
    "item = ds_obqa[\"train\"][0]\n",
    "print(f\"Question: {item['question_stem']}\")\n",
    "print(f\"Choices: {list(zip(item['choices']['label'], item['choices']['text']))}\")\n",
    "print(f\"Answer: {item['answerKey']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "### 5.1 Dataset Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_data = {name: sum(len(v) for v in ds.values())\n",
    "             for name, ds in datasets_map.items()}\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(size_data.keys(), size_data.values(),\n",
    "               color=[\"steelblue\", \"coral\", \"mediumseagreen\", \"orchid\"],\n",
    "               edgecolor=\"white\")\n",
    "plt.title(\"Total Records per Dataset\")\n",
    "plt.ylabel(\"Number of Records\")\n",
    "for bar, val in zip(bars, size_data.values()):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 200,\n",
    "             f\"{val:,}\", ha=\"center\", fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### 5.2 GSM8K: Rationale Analysis\n",
    "\n",
    "In GSM8K, the `answer` field contains step-by-step reasoning followed by\n",
    "`#### <final_answer>`. We parse this to study rationale characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gsm = ds_gsm8k[\"train\"].to_pandas()\n",
    "\n",
    "# Parse rationale and final answer\n",
    "df_gsm[\"rationale\"] = df_gsm[\"answer\"].apply(lambda x: x.split(\"####\")[0].strip())\n",
    "df_gsm[\"final_answer\"] = df_gsm[\"answer\"].apply(\n",
    "    lambda x: x.split(\"####\")[1].strip() if \"####\" in x else \"\"\n",
    ")\n",
    "df_gsm[\"rationale_len\"] = df_gsm[\"rationale\"].apply(len)\n",
    "df_gsm[\"rationale_steps\"] = df_gsm[\"rationale\"].apply(lambda x: x.count(\"\\n\") + 1)\n",
    "df_gsm[\"question_len\"] = df_gsm[\"question\"].apply(len)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "axes[0].hist(df_gsm[\"rationale_steps\"], bins=range(1, 15), color=\"steelblue\",\n",
    "             edgecolor=\"white\", align=\"left\")\n",
    "axes[0].set_title(\"GSM8K: Number of Reasoning Steps\")\n",
    "axes[0].set_xlabel(\"Steps\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1].hist(df_gsm[\"rationale_len\"], bins=50, color=\"coral\", edgecolor=\"white\")\n",
    "axes[1].set_title(\"GSM8K: Rationale Length (chars)\")\n",
    "axes[1].set_xlabel(\"Character Count\")\n",
    "\n",
    "axes[2].scatter(df_gsm[\"question_len\"], df_gsm[\"rationale_len\"],\n",
    "                alpha=0.1, s=5, color=\"mediumseagreen\")\n",
    "axes[2].set_title(\"Question Length vs Rationale Length\")\n",
    "axes[2].set_xlabel(\"Question Length (chars)\")\n",
    "axes[2].set_ylabel(\"Rationale Length (chars)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Reasoning steps - Mean: {df_gsm['rationale_steps'].mean():.1f}, \"\n",
    "      f\"Median: {df_gsm['rationale_steps'].median():.0f}, \"\n",
    "      f\"Max: {df_gsm['rationale_steps'].max()}\")\n",
    "print(f\"Rationale length - Mean: {df_gsm['rationale_len'].mean():.0f} chars, \"\n",
    "      f\"Max: {df_gsm['rationale_len'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### 5.3 AQUA-RAT: Rationale Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aqua = ds_aqua[\"train\"].to_pandas()\n",
    "\n",
    "df_aqua[\"rationale_len\"] = df_aqua[\"rationale\"].apply(len)\n",
    "df_aqua[\"question_len\"] = df_aqua[\"question\"].apply(len)\n",
    "df_aqua[\"rationale_steps\"] = df_aqua[\"rationale\"].apply(lambda x: x.count(\"\\n\") + 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df_aqua[\"rationale_len\"], bins=50, color=\"coral\", edgecolor=\"white\")\n",
    "axes[0].set_title(\"AQUA-RAT: Rationale Length (chars)\")\n",
    "axes[0].set_xlabel(\"Character Count\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "ans_counts = df_aqua[\"correct\"].value_counts().sort_index()\n",
    "axes[1].bar(ans_counts.index, ans_counts.values, color=\"steelblue\",\n",
    "            edgecolor=\"white\")\n",
    "axes[1].set_title(\"AQUA-RAT: Correct Answer Distribution\")\n",
    "axes[1].set_xlabel(\"Answer Choice\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Rationale length - Mean: {df_aqua['rationale_len'].mean():.0f} chars, \"\n",
    "      f\"Max: {df_aqua['rationale_len'].max()}\")\n",
    "print(f\"Rationale steps - Mean: {df_aqua['rationale_steps'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### 5.4 Question Length Comparison Across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_lengths = {\n",
    "    \"GSM8K\": ds_gsm8k[\"train\"].to_pandas()[\"question\"].apply(len),\n",
    "    \"AQUA-RAT\": df_aqua[\"question_len\"],\n",
    "    \"CommonsenseQA\": ds_csqa[\"train\"].to_pandas()[\"question\"].apply(len),\n",
    "    \"OpenBookQA\": ds_obqa[\"train\"].to_pandas()[\"question_stem\"].apply(len),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "colors = [\"steelblue\", \"coral\", \"mediumseagreen\", \"orchid\"]\n",
    "\n",
    "for ax, (name, lengths), color in zip(axes.flat, q_lengths.items(), colors):\n",
    "    ax.hist(lengths, bins=40, color=color, edgecolor=\"white\", alpha=0.8)\n",
    "    ax.set_title(f\"{name}\\nMean={lengths.mean():.0f} chars, \"\n",
    "                 f\"Median={lengths.median():.0f}\")\n",
    "    ax.set_xlabel(\"Character Count\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.suptitle(\"Question Length Distribution Across Datasets\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### 5.5 CommonsenseQA: Answer & Concept Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csqa = ds_csqa[\"train\"].to_pandas()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Answer key distribution\n",
    "ans_counts = df_csqa[\"answerKey\"].value_counts().sort_index()\n",
    "axes[0].bar(ans_counts.index, ans_counts.values, color=\"mediumseagreen\",\n",
    "            edgecolor=\"white\")\n",
    "axes[0].set_title(\"CommonsenseQA: Answer Key Distribution\")\n",
    "axes[0].set_xlabel(\"Answer Key\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Top question concepts\n",
    "top_concepts = df_csqa[\"question_concept\"].value_counts().head(20)\n",
    "axes[1].barh(range(len(top_concepts)), top_concepts.values, color=\"orchid\",\n",
    "             edgecolor=\"white\")\n",
    "axes[1].set_yticks(range(len(top_concepts)))\n",
    "axes[1].set_yticklabels(top_concepts.index, fontsize=8)\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_title(\"Top 20 Question Concepts\")\n",
    "axes[1].set_xlabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Unique question concepts: {df_csqa['question_concept'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### 5.6 CoT Availability: With vs Without Rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_status = pd.DataFrame({\n",
    "    \"Dataset\": [\"GSM8K\", \"AQUA-RAT\", \"CommonsenseQA\", \"OpenBookQA\"],\n",
    "    \"Has Built-in CoT\": [\"Yes (in answer)\", \"Yes (rationale field)\", \"No\", \"No\"],\n",
    "    \"ThoughtSource Adds\": [\"Unified format\", \"Unified format\",\n",
    "                           \"Generated CoT\", \"Generated CoT\"],\n",
    "    \"Domain\": [\"Math\", \"Math\", \"General QA\", \"Science\"],\n",
    "    \"Train Size\": [len(ds_gsm8k[\"train\"]), len(ds_aqua[\"train\"]),\n",
    "                   len(ds_csqa[\"train\"]), len(ds_obqa[\"train\"])],\n",
    "})\n",
    "\n",
    "print(\"=== CoT Availability Summary ===\")\n",
    "print(cot_status.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### 5.7 GSM8K vs AQUA-RAT: Rationale Depth Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Steps comparison\n",
    "axes[0].hist(df_gsm[\"rationale_steps\"], bins=range(1, 15), alpha=0.6,\n",
    "             label=\"GSM8K\", color=\"steelblue\", edgecolor=\"white\", align=\"left\")\n",
    "axes[0].hist(df_aqua[\"rationale_steps\"].clip(upper=14), bins=range(1, 15),\n",
    "             alpha=0.6, label=\"AQUA-RAT\", color=\"coral\", edgecolor=\"white\",\n",
    "             align=\"left\")\n",
    "axes[0].set_title(\"Number of Reasoning Steps\")\n",
    "axes[0].set_xlabel(\"Steps\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Length comparison (normalized)\n",
    "axes[1].hist(df_gsm[\"rationale_len\"], bins=50, alpha=0.6, density=True,\n",
    "             label=\"GSM8K\", color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[1].hist(df_aqua[\"rationale_len\"], bins=50, alpha=0.6, density=True,\n",
    "             label=\"AQUA-RAT\", color=\"coral\", edgecolor=\"white\")\n",
    "axes[1].set_title(\"Rationale Length Distribution (normalized)\")\n",
    "axes[1].set_xlabel(\"Character Count\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Metric\": [\"Mean steps\", \"Median steps\",\n",
    "               \"Mean length (chars)\", \"Median length (chars)\"],\n",
    "    \"GSM8K\": [df_gsm[\"rationale_steps\"].mean(),\n",
    "              df_gsm[\"rationale_steps\"].median(),\n",
    "              df_gsm[\"rationale_len\"].mean(),\n",
    "              df_gsm[\"rationale_len\"].median()],\n",
    "    \"AQUA-RAT\": [df_aqua[\"rationale_steps\"].mean(),\n",
    "                 df_aqua[\"rationale_steps\"].median(),\n",
    "                 df_aqua[\"rationale_len\"].mean(),\n",
    "                 df_aqua[\"rationale_len\"].median()],\n",
    "})\n",
    "\n",
    "print(\"=== Reasoning Depth Comparison ===\")\n",
    "print(comparison.round(1).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 6. Key Observations\n",
    "\n",
    "1. **Unified schema:** ThoughtSource standardizes 14+ datasets into a common\n",
    "   (Question, CoT, Answer) format, enabling cross-domain reasoning research.\n",
    "\n",
    "2. **Explicit vs. generated CoT:** Math datasets (GSM8K, AQUA-RAT) include\n",
    "   human-written rationales, while QA datasets (CommonsenseQA, OpenBookQA)\n",
    "   rely on LLM-generated chains added by ThoughtSource.\n",
    "\n",
    "3. **Reasoning depth varies:** GSM8K averages more structured multi-step\n",
    "   reasoning, while AQUA-RAT rationales tend to be shorter and more formulaic.\n",
    "\n",
    "4. **Scale difference:** AQUA-RAT (~97k) is much larger than GSM8K (~7.5k),\n",
    "   offering quantity vs. quality tradeoffs for training.\n",
    "\n",
    "5. **Research relevance (IS/AI):**\n",
    "   - **Reasoning enhancement:** Fine-tune agents to produce step-by-step rationales\n",
    "   - **Explainability:** Train models that show their work, not just final answers\n",
    "   - **Cross-domain transfer:** Study whether CoT reasoning transfers across domains\n",
    "   - **Human vs. machine reasoning:** Compare human-written and LLM-generated chains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}